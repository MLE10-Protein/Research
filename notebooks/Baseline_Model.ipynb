{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c64a5ac-47f4-488b-afd2-e79a1eac897e",
   "metadata": {},
   "source": [
    "# Model Basline\n",
    "In this notebook we plan to create a continuous-bag-of-words model and feed it into some kind of linear learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7518db13-5a80-475e-9a50-14064afc5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pimports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3564b2e-e909-4e30-99df-9b464f6a62db",
   "metadata": {},
   "source": [
    "## Create CBOW Model\n",
    "\n",
    "To create this with protein data, we will use sequences of 3 amino acids as a word. These are known as k-mers, and here k=3 as is commonly used in research.\n",
    "\n",
    "We will then train the model weights with the kmers (words) surrounding a target kmer (word). From these embeddings we will train a linear learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394afa9d-bca7-4bb0-b015-723c4d7c239f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8aae8838f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Following https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f76523b-6b24-4e6d-9ca5-d379a6046f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequences</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RRWWRRWRRW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GWKSVFRKAKKVGKTVGGLALDHYLG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALWKTMLKKLGTMALHAGKAALGAAADTISQGTQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GLFDVIKKVAAVIGGL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VAKLLAKLAKKVL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>LLKLLKWLLKLLK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>GFKDWIKGAAKKLIKTVAANIANQ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>ESEFDRQEYEECKRQCMQLETSGQMRRCVSQCDKRFEEDIDWSKYDNQE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>FISAIASFLGKFL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>RLSRIVVIRVCR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sequences  label\n",
       "0                                            RRWWRRWRRW      0\n",
       "1                            GWKSVFRKAKKVGKTVGGLALDHYLG      0\n",
       "2                    ALWKTMLKKLGTMALHAGKAALGAAADTISQGTQ      1\n",
       "3                                      GLFDVIKKVAAVIGGL      1\n",
       "4                                         VAKLLAKLAKKVL      1\n",
       "...                                                 ...    ...\n",
       "1373                                      LLKLLKWLLKLLK      0\n",
       "1374                           GFKDWIKGAAKKLIKTVAANIANQ      0\n",
       "1375  ESEFDRQEYEECKRQCMQLETSGQMRRCVSQCDKRFEEDIDWSKYDNQE      0\n",
       "1376                                      FISAIASFLGKFL      1\n",
       "1377                                       RLSRIVVIRVCR      0\n",
       "\n",
       "[1378 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load example protein data to make words\n",
    "prot_df = pd.read_csv(\"data/acp/train_data.csv\")\n",
    "prot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c26a7dc2-6feb-49ba-a5b3-a10b9eb21d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RRW', 17), ('RWW', 5), ('WWR', 4), ('WRR', 11), ('RWR', 10)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn all sequences into kmers (k=3)\n",
    "from collections import Counter  # Might as well count frequency\n",
    "kmers = Counter()\n",
    "for seq in prot_df['sequences'].values:\n",
    "    for kmer_a, kmer_b, kmer_c in zip(seq, seq[1:], seq[2:]):\n",
    "        kmer = ''.join([kmer_a, kmer_b, kmer_c])\n",
    "        kmers[kmer] += 1\n",
    "list(kmers.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da7fc98-3c77-4a45-960d-9cabfd54c455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {kmer: i for i, kmer in enumerate(kmers)}\n",
    "embeds = nn.Embedding(len(word_to_ix), 5)  # x words in vocab, y=5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"RRW\"]], dtype=torch.long)\n",
    "rrw_embed = embeds(lookup_tensor)\n",
    "print(rrw_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebd60897-f98d-4165-95a4-91345deacf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['RRW', 'RWW', 'WRR', 'RRW'], 'WWR'),\n",
       " (['RWW', 'WWR', 'RRW', 'RWR'], 'WRR'),\n",
       " (['WWR', 'WRR', 'RWR', 'WRR'], 'RRW'),\n",
       " (['WRR', 'RRW', 'WRR', 'RRW'], 'RWR'),\n",
       " (['GWK', 'WKS', 'SVF', 'VFR'], 'KSV')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "vocab = set(kmers)\n",
    "vocab_size = len(vocab)\n",
    "kmer_len = 3\n",
    "\n",
    "data = []\n",
    "\n",
    "for seq in prot_df['sequences'].values:\n",
    "    for i in range(0, len(seq) - CONTEXT_SIZE*kmer_len):\n",
    "        kmer_minus_2 = seq[i:i+kmer_len]\n",
    "        kmer_minus_1 = seq[i+1:i+kmer_len+1]\n",
    "        kmer_i = seq[i+2:i+kmer_len+2]\n",
    "        kmer_plus_1 = seq[i+3:i+kmer_len+3]\n",
    "        kmer_plus_2 = seq[i+4:i+kmer_len+4]\n",
    "        data.append(([kmer_minus_2, kmer_minus_1, kmer_plus_1, kmer_plus_2], kmer_i))\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5bb9f0cf-3780-4c92-922c-43c6d03e7c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 24582 rows\n",
      "Removing 0 contexts\n",
      "Now have 24582\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "print(f'Starting with {len(data)} rows')\n",
    "dupes = {}\n",
    "for ctx, wrd in data:\n",
    "    ctx = ','.join(ctx)\n",
    "    if ctx not in dupes:\n",
    "        dupes[ctx] = {wrd}\n",
    "    else:\n",
    "        dupes[ctx].add(wrd)\n",
    "duplicates = {ctx for ctx, wrds in dupes.items() if len(wrds) > 1}\n",
    "print(f'Removing {len(duplicates):,} contexts')\n",
    "\n",
    "data = [(ctx, word) for ctx, word in data if ','.join(ctx) not in duplicates]\n",
    "print(f'Now have {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e825ce99-a346-4d83-a843-e6b3a1bd946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 2**7)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(2**7, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad46ce42-b2a3-4dff-a9b8-941ca4495f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(5647, 100)\n",
       "  (linear1): Linear(in_features=100, out_features=128, bias=True)\n",
       "  (activation_function1): ReLU()\n",
       "  (linear2): Linear(in_features=128, out_features=5647, bias=True)\n",
       "  (activation_function2): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CBOW(vocab_size, 100)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37a75e60-e70f-4ab4-91c5-7040c375d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736155b7-f458-4ee4-b8f8-9a4b40f9e605",
   "metadata": {},
   "source": [
    "I played with the optimizer a lot. With a large lr and SGD, I got NaN loss reliably. I decided to avoid this by using Adam optimizer which allows for weight decay, which tries to minimize the weights. I also found that higher learning rate is fine for a baseline model, but not for a very good one, as there will be lots of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34bf6d4e-0b97-44f3-bf49-f06c3b745d90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "tensor(11826.0049, grad_fn=<AddBackward0>)\n",
      "Done with epoch 0 and got accuracy 1616/4917=32.86557%\n",
      "Starting epoch 1\n",
      "tensor(17810.7676, grad_fn=<AddBackward0>)\n",
      "Done with epoch 1 and got accuracy 1387/4917=28.20826%\n",
      "Starting epoch 2\n",
      "tensor(6046.8428, grad_fn=<AddBackward0>)\n",
      "Done with epoch 2 and got accuracy 1642/4917=33.39435%\n",
      "Starting epoch 3\n",
      "tensor(5740.8506, grad_fn=<AddBackward0>)\n",
      "Done with epoch 3 and got accuracy 1673/4917=34.02481%\n",
      "Starting epoch 4\n",
      "tensor(5520.6245, grad_fn=<AddBackward0>)\n",
      "Done with epoch 4 and got accuracy 1681/4917=34.18751%\n",
      "Starting epoch 5\n",
      "tensor(4224.8203, grad_fn=<AddBackward0>)\n",
      "Done with epoch 5 and got accuracy 1727/4917=35.12304%\n",
      "Starting epoch 6\n",
      "tensor(2669.7395, grad_fn=<AddBackward0>)\n",
      "Done with epoch 6 and got accuracy 1787/4917=36.34330%\n",
      "Starting epoch 7\n",
      "tensor(1649.0242, grad_fn=<AddBackward0>)\n",
      "Done with epoch 7 and got accuracy 1824/4917=37.09579%\n",
      "Starting epoch 8\n",
      "tensor(1032.1907, grad_fn=<AddBackward0>)\n",
      "Done with epoch 8 and got accuracy 1854/4917=37.70592%\n",
      "Starting epoch 9\n",
      "tensor(690.3312, grad_fn=<AddBackward0>)\n",
      "Done with epoch 9 and got accuracy 1855/4917=37.72626%\n",
      "Starting epoch 10\n",
      "tensor(491.3072, grad_fn=<AddBackward0>)\n",
      "Done with epoch 10 and got accuracy 1866/4917=37.94997%\n",
      "Starting epoch 11\n",
      "tensor(334.4634, grad_fn=<AddBackward0>)\n",
      "Done with epoch 11 and got accuracy 1873/4917=38.09233%\n",
      "Starting epoch 12\n",
      "tensor(219.4837, grad_fn=<AddBackward0>)\n",
      "Done with epoch 12 and got accuracy 1875/4917=38.13301%\n",
      "Starting epoch 13\n",
      "tensor(158.9024, grad_fn=<AddBackward0>)\n",
      "Done with epoch 13 and got accuracy 1880/4917=38.23470%\n",
      "Starting epoch 14\n",
      "tensor(124.8635, grad_fn=<AddBackward0>)\n",
      "Done with epoch 14 and got accuracy 1882/4917=38.27537%\n",
      "Starting epoch 15\n",
      "tensor(104.4582, grad_fn=<AddBackward0>)\n",
      "Done with epoch 15 and got accuracy 1882/4917=38.27537%\n",
      "Starting epoch 16\n",
      "tensor(87.5079, grad_fn=<AddBackward0>)\n",
      "Done with epoch 16 and got accuracy 1886/4917=38.35672%\n",
      "Starting epoch 17\n",
      "tensor(79.0022, grad_fn=<AddBackward0>)\n",
      "Done with epoch 17 and got accuracy 1879/4917=38.21436%\n",
      "Starting epoch 18\n",
      "tensor(65.4491, grad_fn=<AddBackward0>)\n",
      "Done with epoch 18 and got accuracy 1880/4917=38.23470%\n",
      "Starting epoch 19\n",
      "tensor(54.5224, grad_fn=<AddBackward0>)\n",
      "Done with epoch 19 and got accuracy 1879/4917=38.21436%\n",
      "Starting epoch 20\n",
      "tensor(50.5455, grad_fn=<AddBackward0>)\n",
      "Done with epoch 20 and got accuracy 1880/4917=38.23470%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#optimize at the end of each epoch\u001b[39;00m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.04, weight_decay=0.001)\n",
    "num_train = int(0.8*len(data))\n",
    "num_test = len(data) - num_train\n",
    "train_data, test_data = torch.utils.data.random_split(data, [num_train, num_test], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "#TRAINING\n",
    "for epoch in range(100):\n",
    "    print(f'Starting epoch {epoch}')\n",
    "    total_loss = 0\n",
    "\n",
    "    for context, target in train_data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)\n",
    "        log_probs = model(context_vector)\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
    "    print(total_loss)\n",
    "    if np.isnan(float(f'{total_loss}')):\n",
    "        break\n",
    "\n",
    "    correct_guesses = 0\n",
    "    for context, target in test_data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)\n",
    "        log_probs = model(context_vector)\n",
    "        guessed = ix_to_word[torch.argmax(log_probs[0]).item()]\n",
    "        if guessed == target:\n",
    "            correct_guesses += 1\n",
    "    print(f'Done with epoch {epoch} and got accuracy {correct_guesses}/{len(test_data)}={100*(correct_guesses / len(test_data)):.5f}%')\n",
    "\n",
    "    #optimize at the end of each epoch\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9bbc1aa1-fabc-4ceb-8907-a09503ddb7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with training and got test accuracy 1880/4917=38.23470% and train accuracy 19647/19665=99.90847%\n"
     ]
    }
   ],
   "source": [
    "# Final loss and accuracy\n",
    "train_guesses = 0\n",
    "for context, target in train_data:\n",
    "    context_vector = make_context_vector(context, word_to_ix)\n",
    "    log_probs = model(context_vector)\n",
    "    guessed = ix_to_word[torch.argmax(log_probs[0]).item()]\n",
    "    if guessed == target:\n",
    "        train_guesses += 1\n",
    "\n",
    "correct_guesses = 0\n",
    "for context, target in test_data:\n",
    "    context_vector = make_context_vector(context, word_to_ix)\n",
    "    log_probs = model(context_vector)\n",
    "    guessed = ix_to_word[torch.argmax(log_probs[0]).item()]\n",
    "    if guessed == target:\n",
    "        correct_guesses += 1\n",
    "print(f'Done with training and got test accuracy {correct_guesses}/{len(test_data)}={100*(correct_guesses / len(test_data)):.5f}% and train accuracy {train_guesses}/{len(train_data)}={100*(train_guesses / len(train_data)):.5f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad03556-6c7a-470e-aafa-9be1d697aa10",
   "metadata": {},
   "source": [
    "38% test accuracy and 99% train accuracy :D Smells like overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc1bd4-80ef-40b4-a33e-cefb84707b1a",
   "metadata": {},
   "source": [
    "## Linear model for guessing label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "730f9cff-376a-4eb4-8256-6e3e2cdb8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(seq):\n",
    "    # Convert to kmers and lookup\n",
    "    kmers = []\n",
    "    for i in range(0, len(seq) - 2):\n",
    "        kmer = seq[i:i+3]\n",
    "        kmers.append(kmer)\n",
    "    return model.embeddings(torch.tensor([word_to_ix[kmer] for kmer in kmers])).cpu().detach().numpy()\n",
    "\n",
    "# Test\n",
    "# get_embeddings('RRWR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d659b8b9-b184-445b-a830-74a2a64c0691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_x</th>\n",
       "      <th>1_x</th>\n",
       "      <th>2_x</th>\n",
       "      <th>3_x</th>\n",
       "      <th>4_x</th>\n",
       "      <th>5_x</th>\n",
       "      <th>6_x</th>\n",
       "      <th>7_x</th>\n",
       "      <th>8_x</th>\n",
       "      <th>9_x</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.444002</td>\n",
       "      <td>0.056918</td>\n",
       "      <td>-0.815464</td>\n",
       "      <td>0.573925</td>\n",
       "      <td>-0.183249</td>\n",
       "      <td>-0.246367</td>\n",
       "      <td>0.569723</td>\n",
       "      <td>0.364270</td>\n",
       "      <td>0.693070</td>\n",
       "      <td>-0.056975</td>\n",
       "      <td>...</td>\n",
       "      <td>7.795949</td>\n",
       "      <td>3.839921</td>\n",
       "      <td>-2.149093</td>\n",
       "      <td>-6.283219</td>\n",
       "      <td>-7.843275</td>\n",
       "      <td>4.828357</td>\n",
       "      <td>3.948138</td>\n",
       "      <td>0.419173</td>\n",
       "      <td>8.552602</td>\n",
       "      <td>-1.960219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.126992</td>\n",
       "      <td>0.049871</td>\n",
       "      <td>0.606737</td>\n",
       "      <td>-0.144443</td>\n",
       "      <td>0.298717</td>\n",
       "      <td>-0.233551</td>\n",
       "      <td>-0.267371</td>\n",
       "      <td>-0.264926</td>\n",
       "      <td>-0.069222</td>\n",
       "      <td>0.448553</td>\n",
       "      <td>...</td>\n",
       "      <td>6.313254</td>\n",
       "      <td>-2.015187</td>\n",
       "      <td>1.417841</td>\n",
       "      <td>9.451605</td>\n",
       "      <td>-5.477207</td>\n",
       "      <td>0.024754</td>\n",
       "      <td>-3.430053</td>\n",
       "      <td>-0.231232</td>\n",
       "      <td>-2.715113</td>\n",
       "      <td>7.010417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.223886</td>\n",
       "      <td>0.235404</td>\n",
       "      <td>-0.050235</td>\n",
       "      <td>0.462699</td>\n",
       "      <td>0.066580</td>\n",
       "      <td>-0.155796</td>\n",
       "      <td>-0.264497</td>\n",
       "      <td>0.078733</td>\n",
       "      <td>-0.048532</td>\n",
       "      <td>0.270114</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.322323</td>\n",
       "      <td>6.068357</td>\n",
       "      <td>-3.053094</td>\n",
       "      <td>-1.743851</td>\n",
       "      <td>-0.013190</td>\n",
       "      <td>-11.695951</td>\n",
       "      <td>-7.152821</td>\n",
       "      <td>9.071996</td>\n",
       "      <td>0.459658</td>\n",
       "      <td>-0.156087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.556409</td>\n",
       "      <td>-0.207510</td>\n",
       "      <td>0.029089</td>\n",
       "      <td>0.204503</td>\n",
       "      <td>0.083392</td>\n",
       "      <td>0.393404</td>\n",
       "      <td>-0.198617</td>\n",
       "      <td>0.778663</td>\n",
       "      <td>0.161003</td>\n",
       "      <td>-0.387152</td>\n",
       "      <td>...</td>\n",
       "      <td>1.107438</td>\n",
       "      <td>-5.195529</td>\n",
       "      <td>0.497815</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>-1.462043</td>\n",
       "      <td>7.551419</td>\n",
       "      <td>3.362416</td>\n",
       "      <td>5.714870</td>\n",
       "      <td>-0.686924</td>\n",
       "      <td>-8.062219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.145889</td>\n",
       "      <td>0.275106</td>\n",
       "      <td>0.067376</td>\n",
       "      <td>-0.089389</td>\n",
       "      <td>-0.116227</td>\n",
       "      <td>-0.378428</td>\n",
       "      <td>-0.191439</td>\n",
       "      <td>0.203924</td>\n",
       "      <td>-0.131348</td>\n",
       "      <td>0.213792</td>\n",
       "      <td>...</td>\n",
       "      <td>3.427410</td>\n",
       "      <td>4.751172</td>\n",
       "      <td>1.452300</td>\n",
       "      <td>5.397296</td>\n",
       "      <td>-2.089801</td>\n",
       "      <td>-1.887212</td>\n",
       "      <td>3.218089</td>\n",
       "      <td>4.966340</td>\n",
       "      <td>-1.001770</td>\n",
       "      <td>-0.731229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>-0.496204</td>\n",
       "      <td>0.157467</td>\n",
       "      <td>0.307309</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>-0.306633</td>\n",
       "      <td>-0.789912</td>\n",
       "      <td>0.297118</td>\n",
       "      <td>0.385418</td>\n",
       "      <td>-0.273252</td>\n",
       "      <td>0.047054</td>\n",
       "      <td>...</td>\n",
       "      <td>3.260774</td>\n",
       "      <td>10.666686</td>\n",
       "      <td>5.799181</td>\n",
       "      <td>-3.966213</td>\n",
       "      <td>5.669536</td>\n",
       "      <td>-2.449130</td>\n",
       "      <td>3.074576</td>\n",
       "      <td>-2.413268</td>\n",
       "      <td>-4.381186</td>\n",
       "      <td>1.743143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>0.009337</td>\n",
       "      <td>-0.427140</td>\n",
       "      <td>0.381318</td>\n",
       "      <td>0.053133</td>\n",
       "      <td>0.120318</td>\n",
       "      <td>0.055844</td>\n",
       "      <td>-0.139122</td>\n",
       "      <td>0.093994</td>\n",
       "      <td>0.039429</td>\n",
       "      <td>0.157610</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.919102</td>\n",
       "      <td>6.566476</td>\n",
       "      <td>-5.235083</td>\n",
       "      <td>-5.783195</td>\n",
       "      <td>-3.001599</td>\n",
       "      <td>-4.548702</td>\n",
       "      <td>-2.995428</td>\n",
       "      <td>-2.787709</td>\n",
       "      <td>-0.486865</td>\n",
       "      <td>1.645817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>-0.151590</td>\n",
       "      <td>0.133433</td>\n",
       "      <td>0.162137</td>\n",
       "      <td>0.189756</td>\n",
       "      <td>0.209770</td>\n",
       "      <td>0.046477</td>\n",
       "      <td>-0.192852</td>\n",
       "      <td>-0.224525</td>\n",
       "      <td>-0.338736</td>\n",
       "      <td>0.124081</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.011648</td>\n",
       "      <td>-10.000915</td>\n",
       "      <td>-12.864613</td>\n",
       "      <td>-10.397711</td>\n",
       "      <td>10.204558</td>\n",
       "      <td>-11.258795</td>\n",
       "      <td>8.158518</td>\n",
       "      <td>8.305094</td>\n",
       "      <td>3.359185</td>\n",
       "      <td>-8.301657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>0.188698</td>\n",
       "      <td>0.049085</td>\n",
       "      <td>0.467198</td>\n",
       "      <td>-0.159195</td>\n",
       "      <td>0.121160</td>\n",
       "      <td>0.396945</td>\n",
       "      <td>-0.483380</td>\n",
       "      <td>0.266270</td>\n",
       "      <td>0.282483</td>\n",
       "      <td>0.095831</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.366853</td>\n",
       "      <td>2.664019</td>\n",
       "      <td>-1.250323</td>\n",
       "      <td>1.958580</td>\n",
       "      <td>-2.589087</td>\n",
       "      <td>-1.681041</td>\n",
       "      <td>5.271566</td>\n",
       "      <td>0.630387</td>\n",
       "      <td>-3.710766</td>\n",
       "      <td>-3.578943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>0.227965</td>\n",
       "      <td>-0.405863</td>\n",
       "      <td>-0.235844</td>\n",
       "      <td>0.505946</td>\n",
       "      <td>-0.086601</td>\n",
       "      <td>-0.138654</td>\n",
       "      <td>0.251235</td>\n",
       "      <td>-0.085655</td>\n",
       "      <td>-0.689960</td>\n",
       "      <td>-0.043371</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.713783</td>\n",
       "      <td>5.572189</td>\n",
       "      <td>-2.972149</td>\n",
       "      <td>-3.912648</td>\n",
       "      <td>3.083359</td>\n",
       "      <td>-2.327823</td>\n",
       "      <td>3.317887</td>\n",
       "      <td>3.862004</td>\n",
       "      <td>1.920710</td>\n",
       "      <td>-3.166190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0_x       1_x       2_x       3_x       4_x       5_x       6_x  \\\n",
       "0     0.444002  0.056918 -0.815464  0.573925 -0.183249 -0.246367  0.569723   \n",
       "1     0.126992  0.049871  0.606737 -0.144443  0.298717 -0.233551 -0.267371   \n",
       "2    -0.223886  0.235404 -0.050235  0.462699  0.066580 -0.155796 -0.264497   \n",
       "3     0.556409 -0.207510  0.029089  0.204503  0.083392  0.393404 -0.198617   \n",
       "4     0.145889  0.275106  0.067376 -0.089389 -0.116227 -0.378428 -0.191439   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1373 -0.496204  0.157467  0.307309  0.005698 -0.306633 -0.789912  0.297118   \n",
       "1374  0.009337 -0.427140  0.381318  0.053133  0.120318  0.055844 -0.139122   \n",
       "1375 -0.151590  0.133433  0.162137  0.189756  0.209770  0.046477 -0.192852   \n",
       "1376  0.188698  0.049085  0.467198 -0.159195  0.121160  0.396945 -0.483380   \n",
       "1377  0.227965 -0.405863 -0.235844  0.505946 -0.086601 -0.138654  0.251235   \n",
       "\n",
       "           7_x       8_x       9_x  ...        90         91         92  \\\n",
       "0     0.364270  0.693070 -0.056975  ...  7.795949   3.839921  -2.149093   \n",
       "1    -0.264926 -0.069222  0.448553  ...  6.313254  -2.015187   1.417841   \n",
       "2     0.078733 -0.048532  0.270114  ... -5.322323   6.068357  -3.053094   \n",
       "3     0.778663  0.161003 -0.387152  ...  1.107438  -5.195529   0.497815   \n",
       "4     0.203924 -0.131348  0.213792  ...  3.427410   4.751172   1.452300   \n",
       "...        ...       ...       ...  ...       ...        ...        ...   \n",
       "1373  0.385418 -0.273252  0.047054  ...  3.260774  10.666686   5.799181   \n",
       "1374  0.093994  0.039429  0.157610  ... -6.919102   6.566476  -5.235083   \n",
       "1375 -0.224525 -0.338736  0.124081  ... -2.011648 -10.000915 -12.864613   \n",
       "1376  0.266270  0.282483  0.095831  ... -6.366853   2.664019  -1.250323   \n",
       "1377 -0.085655 -0.689960 -0.043371  ... -5.713783   5.572189  -2.972149   \n",
       "\n",
       "             93         94         95        96        97        98        99  \n",
       "0     -6.283219  -7.843275   4.828357  3.948138  0.419173  8.552602 -1.960219  \n",
       "1      9.451605  -5.477207   0.024754 -3.430053 -0.231232 -2.715113  7.010417  \n",
       "2     -1.743851  -0.013190 -11.695951 -7.152821  9.071996  0.459658 -0.156087  \n",
       "3      0.000678  -1.462043   7.551419  3.362416  5.714870 -0.686924 -8.062219  \n",
       "4      5.397296  -2.089801  -1.887212  3.218089  4.966340 -1.001770 -0.731229  \n",
       "...         ...        ...        ...       ...       ...       ...       ...  \n",
       "1373  -3.966213   5.669536  -2.449130  3.074576 -2.413268 -4.381186  1.743143  \n",
       "1374  -5.783195  -3.001599  -4.548702 -2.995428 -2.787709 -0.486865  1.645817  \n",
       "1375 -10.397711  10.204558 -11.258795  8.158518  8.305094  3.359185 -8.301657  \n",
       "1376   1.958580  -2.589087  -1.681041  5.271566  0.630387 -3.710766 -3.578943  \n",
       "1377  -3.912648   3.083359  -2.327823  3.317887  3.862004  1.920710 -3.166190  \n",
       "\n",
       "[1378 rows x 300 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prot_df['embeddings'] = prot_df['sequences'].map(get_embeddings)\n",
    "\n",
    "prot_df['mean_embeddings'] = prot_df['embeddings'].map(lambda x: x.mean(0))\n",
    "prot_df['max_embeddings'] = prot_df['embeddings'].map(lambda x: x.max(0))\n",
    "prot_df['sum_embeddings'] = prot_df['embeddings'].map(lambda x: x.sum(0))\n",
    "\n",
    "mean_embeddings = pd.DataFrame(prot_df['mean_embeddings'].tolist())\n",
    "max_embeddings = pd.DataFrame(prot_df['max_embeddings'].tolist())\n",
    "sum_embeddings = pd.DataFrame(prot_df['sum_embeddings'].tolist())\n",
    "all_embeddings = mean_embeddings.merge(max_embeddings, left_index=True, right_index=True).merge(sum_embeddings, left_index=True, right_index=True)\n",
    "all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8439a-da04-4a54-84ce-ad5ad9a6bb0f",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8ddbcff-638d-44f9-b0d2-264c9c310a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jong/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72       150\n",
      "           1       0.67      0.68      0.67       126\n",
      "\n",
      "    accuracy                           0.70       276\n",
      "   macro avg       0.70      0.70      0.70       276\n",
      "weighted avg       0.70      0.70      0.70       276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jong/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=1_000, max_depth=10, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = all_embeddings, prot_df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit RandomForestClassifier\n",
    "rfc.fit(X_train, y_train)\n",
    "# Predict the test set labels\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Analyze results\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d').set_title('confusion matrix')\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeef61d-2100-4c3a-b9c5-cfbc4de32622",
   "metadata": {},
   "source": [
    "Around 70% accuracy. That's pretty good for a baseline! Especially with a very overfitting embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa2a14-de29-4263-b165-8f422bb00ab3",
   "metadata": {},
   "source": [
    "## Lets create the same results for the other two protein datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6f67b73-9b41-4f9f-bac8-0b719a490387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to support OOV (Out Of Vocab)\n",
    "\n",
    "def get_embeddings_with_oov(seq):\n",
    "    # Convert to kmers and lookup\n",
    "    kmers = []\n",
    "    for i in range(0, len(seq) - 2):\n",
    "        kmer = seq[i:i+3]\n",
    "        kmers.append(kmer)\n",
    "    return model.embeddings(torch.tensor([word_to_ix[kmer] for kmer in kmers if kmer in word_to_ix])).cpu().detach().numpy()\n",
    "\n",
    "# Test\n",
    "# get_embeddings('RRWR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9d35ce2-4361-4509-aaa6-3f09ad1f9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_df(df):\n",
    "    df['embeddings'] = df['sequences'].map(get_embeddings_with_oov)\n",
    "\n",
    "    df['mean_embeddings'] = df['embeddings'].map(lambda x: x.mean(0))\n",
    "    df['max_embeddings'] = df['embeddings'].map(lambda x: x.max(0))\n",
    "    df['sum_embeddings'] = df['embeddings'].map(lambda x: x.sum(0))\n",
    "\n",
    "    mean_embeddings = pd.DataFrame(df['mean_embeddings'].tolist())\n",
    "    max_embeddings = pd.DataFrame(df['max_embeddings'].tolist())\n",
    "    sum_embeddings = pd.DataFrame(df['sum_embeddings'].tolist())\n",
    "    all_embeddings = mean_embeddings.merge(max_embeddings, left_index=True, right_index=True).merge(sum_embeddings, left_index=True, right_index=True)\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e1d2bb0-ff12-42e5-987a-eeb72ac5578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_run_rf(all_embeddings, label_df):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rfc = RandomForestClassifier(\n",
    "        n_estimators=1_000, max_depth=10, random_state=42)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X, y = all_embeddings, label_df['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit RandomForestClassifier\n",
    "    rfc.fit(X_train, y_train)\n",
    "    # Predict the test set labels\n",
    "    y_pred = rfc.predict(X_test)\n",
    "\n",
    "    # Analyze results\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d').set_title('confusion matrix')\n",
    "\n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc4d39-22e9-48cb-821e-16e079d01ba0",
   "metadata": {},
   "source": [
    "### AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "08586392-2344-4723-930c-e176f38fbb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jong/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/home/jong/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.69       382\n",
      "           1       0.73      0.70      0.71       427\n",
      "\n",
      "    accuracy                           0.70       809\n",
      "   macro avg       0.70      0.70      0.70       809\n",
      "weighted avg       0.70      0.70      0.70       809\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "amp_df = pd.read_csv('data/amp/all_data.csv').rename(columns={'SequenceID': 'sequences'})\n",
    "get_and_run_rf(get_embedding_df(amp_df), amp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0584ce-6fc0-49c6-beab-bae35270503a",
   "metadata": {},
   "source": [
    "Wow, still 70% accuracy, even with many words OOV!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad45ca-d1ba-41c8-b371-d78d19f347cb",
   "metadata": {},
   "source": [
    "### DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6c2a7a0a-88cb-45c5-aa27-304221a15079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jong/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/home/jong/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      1410\n",
      "           1       0.75      0.64      0.69      1428\n",
      "\n",
      "    accuracy                           0.71      2838\n",
      "   macro avg       0.71      0.71      0.71      2838\n",
      "weighted avg       0.71      0.71      0.71      2838\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dna_df = pd.read_csv('data/dna_binding/train.csv').rename(columns={'sequence': 'sequences'})\n",
    "get_and_run_rf(get_embedding_df(dna_df), dna_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa802b5-44bb-4ce1-9181-d439515dc5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
