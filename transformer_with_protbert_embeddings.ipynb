{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d8db45-849d-41e6-9240-767f67d957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "NUM_CLASSES=1_000\n",
    "BATCH_SIZE = 2**6\n",
    "MAX_SEQ_LENGTH = 256\n",
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0517f53-d48c-4f94-a758-809124c81f45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext in ./.local/lib/python3.8/site-packages (0.14.1)\n",
      "Requirement already satisfied: pyarrow in ./.local/lib/python3.8/site-packages (10.0.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.8/site-packages (4.25.1)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from torchtext) (4.64.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from torchtext) (1.23.4)\n",
      "Requirement already satisfied: torch==1.13.1 in ./.local/lib/python3.8/site-packages (from torchtext) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.local/lib/python3.8/site-packages (from torch==1.13.1->torchtext) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.local/lib/python3.8/site-packages (from torch==1.13.1->torchtext) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.8/site-packages (from torch==1.13.1->torchtext) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.local/lib/python3.8/site-packages (from torch==1.13.1->torchtext) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.local/lib/python3.8/site-packages (from torch==1.13.1->torchtext) (11.7.99)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchtext) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchtext) (45.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.local/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in ./.local/lib/python3.8/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchtext) (1.25.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.local/lib/python3.8/site-packages (from requests->torchtext) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchtext) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchtext) (2.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext pyarrow transformers structlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97dfadde-6359-4359-b514-fd30c16b3bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import gc\n",
    "import structlog\n",
    "\n",
    "GPU = True\n",
    "device = torch.device(\"cuda\" if GPU else \"cpu\")\n",
    "data_path = '/home/ubuntu/'\n",
    "logger = structlog.getLogger()\n",
    "logger.info(f\"Getting started with {GPU=} {device=} {data_path=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ac24e0-d77c-42b5-bba7-f7ec70be4a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/ubuntu/\u001b[00m\n",
      "├── \u001b[01;31mparquet_data.zip\u001b[00m\n",
      "├── \u001b[01;34msnap\u001b[00m\n",
      "│   └── \u001b[01;34mnvtop\u001b[00m\n",
      "│       ├── \u001b[01;34m66\u001b[00m\n",
      "│       ├── \u001b[01;34mcommon\u001b[00m\n",
      "│       └── \u001b[01;36mcurrent\u001b[00m -> \u001b[01;34m66\u001b[00m\n",
      "├── test_df.parquet\n",
      "├── train_df.parquet\n",
      "├── try1.ipynb\n",
      "└── val_df.parquet\n",
      "\n",
      "5 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree /home/ubuntu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd25c11f-dcc0-4400-aca8-500e3e58a6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1086741, 5), (126171, 5), (126171, 5))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_parquet(data_path + 'train_df.parquet')\n",
    "test_df = pd.read_parquet(data_path + 'test_df.parquet')\n",
    "val_df = pd.read_parquet(data_path + 'val_df.parquet')\n",
    "train_df.shape, test_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c47a72-5601-443e-b947-3b9bb9ba9e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.13.1+cu117', '0.14.1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1757ff69-2684-4b11-9bf5-9f6a5f8a7f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[-1.8532e+01, -1.9899e+01, -1.9202e+01, -1.9233e+01, -2.1818e+01,\n",
       "          -3.3183e-01, -4.3617e-01, -1.6150e-01, -2.5575e-02, -1.2855e+00,\n",
       "           4.6539e-01, -1.3698e+00,  2.5481e-02,  1.3961e+00, -2.3760e+00,\n",
       "          -2.5161e-01, -6.2316e-01, -9.2688e-01, -8.8103e-01, -1.4678e+00,\n",
       "          -1.6217e+00,  3.2145e+00, -2.0604e+00, -4.5068e-01,  4.5483e-01,\n",
       "          -3.9251e+00, -1.8108e+01, -1.8154e+01, -1.8443e+01, -1.8931e+01],\n",
       "         [-2.1589e+01, -2.1636e+01, -2.1418e+01, -2.1295e+01, -2.2876e+01,\n",
       "          -8.4695e-01, -8.1240e-01,  7.8902e-01, -1.7455e+00, -1.4653e+00,\n",
       "           5.6490e-01, -1.1674e+00,  1.2356e+00,  4.1438e+00, -2.4756e+00,\n",
       "           2.0495e+00, -1.3056e+00,  7.8864e-01,  1.2412e+00, -2.7056e+00,\n",
       "          -1.4608e+00, -3.3685e-01, -2.7666e+00,  2.4256e-01,  3.9161e+00,\n",
       "          -6.9252e+00, -2.1043e+01, -2.1170e+01, -2.0820e+01, -2.1201e+01],\n",
       "         [-2.1355e+01, -2.0936e+01, -2.0803e+01, -1.9996e+01, -2.2508e+01,\n",
       "          -1.0702e+00, -1.2807e+00,  8.0083e-01, -2.5328e+00, -2.0794e+00,\n",
       "          -7.4695e-02, -2.3765e+00, -8.2487e-02,  7.4417e+00, -2.5173e+00,\n",
       "          -2.1427e-01, -4.9526e-01, -8.6128e-01,  4.1846e-01, -2.9880e+00,\n",
       "          -2.2347e+00, -1.0304e+00, -2.0541e+00,  3.1538e-01,  3.7330e+00,\n",
       "          -6.5409e+00, -2.0793e+01, -2.0568e+01, -2.0238e+01, -2.0369e+01],\n",
       "         [-2.0309e+01, -2.1225e+01, -2.0786e+01, -2.1611e+01, -2.2968e+01,\n",
       "          -6.3732e-01, -1.3872e+00,  5.2063e-01, -1.9765e+00, -2.2919e+00,\n",
       "           2.1752e-01, -1.3139e+00, -3.5396e-01,  4.5471e+00, -2.6896e+00,\n",
       "          -4.1946e-01, -7.7500e-01, -9.1249e-01, -4.1802e-01, -1.5727e+00,\n",
       "          -1.8177e+00, -1.3429e+00, -1.4203e+00,  1.6126e+00,  6.4946e+00,\n",
       "          -6.4191e+00, -2.1311e+01, -2.0745e+01, -2.0646e+01, -2.0351e+01],\n",
       "         [-2.0537e+01, -2.1130e+01, -2.0580e+01, -2.1501e+01, -2.3339e+01,\n",
       "          -1.2329e-01, -1.4036e+00, -4.4960e-01, -1.8578e+00, -2.2360e+00,\n",
       "           2.5257e-01, -1.3333e+00,  3.5623e-01,  3.7940e+00, -3.2670e+00,\n",
       "           5.4703e-01, -3.9180e-01, -4.6327e-01,  2.8046e-01, -1.2187e+00,\n",
       "          -1.7014e+00, -4.2954e-01, -2.4735e+00,  5.4795e-01,  7.8722e+00,\n",
       "          -6.0718e+00, -2.0745e+01, -2.0331e+01, -2.0259e+01, -2.0217e+01],\n",
       "         [-2.1681e+01, -2.1723e+01, -2.1198e+01, -2.0857e+01, -2.3232e+01,\n",
       "          -4.7492e-01, -1.7424e+00, -8.1062e-01, -2.1330e+00, -2.0256e+00,\n",
       "           5.8700e-01, -6.6166e-01,  2.3291e+00,  4.6294e+00, -3.7499e+00,\n",
       "           1.4300e+00, -1.6061e+00,  8.1381e-01,  2.5701e+00, -2.5303e+00,\n",
       "          -1.6739e+00,  5.9004e-01, -2.6787e+00, -2.1176e-01,  4.4382e+00,\n",
       "          -6.2795e+00, -2.0782e+01, -2.0877e+01, -2.0428e+01, -2.0842e+01],\n",
       "         [-2.1463e+01, -2.0966e+01, -2.1074e+01, -2.0067e+01, -2.2693e+01,\n",
       "          -6.2267e-01, -7.8163e-01, -9.2801e-02, -2.3375e+00, -1.6929e+00,\n",
       "          -3.6896e-01, -2.4060e+00,  2.4572e-01,  7.2160e+00, -2.4930e+00,\n",
       "           3.9765e-03, -8.6921e-01, -7.7966e-01,  6.9086e-01, -2.8618e+00,\n",
       "          -2.5252e+00, -6.8937e-01, -2.5183e+00, -3.7424e-01,  3.2148e+00,\n",
       "          -6.9142e+00, -2.0618e+01, -2.0634e+01, -2.0460e+01, -2.0323e+01],\n",
       "         [-2.1726e+01, -2.1593e+01, -2.1576e+01, -1.9898e+01, -2.3151e+01,\n",
       "          -5.6528e-01, -1.4556e+00,  3.5838e-01, -2.6066e+00, -2.4335e+00,\n",
       "           3.1791e-01, -2.3757e+00, -7.8839e-01,  6.8987e+00, -2.9696e+00,\n",
       "          -3.6685e-01, -4.6606e-01, -1.2922e+00, -1.1873e-01, -1.9211e+00,\n",
       "          -1.7726e+00, -1.3445e+00, -2.2835e+00,  1.2320e+00,  4.1059e+00,\n",
       "          -6.9427e+00, -2.1245e+01, -2.1350e+01, -2.0804e+01, -2.1145e+01],\n",
       "         [-2.1187e+01, -2.0810e+01, -2.0729e+01, -2.0024e+01, -2.2602e+01,\n",
       "          -1.3754e+00, -1.4733e+00,  6.6327e-01, -2.4095e+00, -2.2636e+00,\n",
       "          -9.5332e-02, -2.3874e+00, -6.3662e-01,  6.8972e+00, -2.7144e+00,\n",
       "          -2.5882e-01, -6.1134e-01, -9.7689e-01, -6.1487e-02, -2.5422e+00,\n",
       "          -1.5595e+00, -1.1254e+00, -2.4075e+00,  1.1938e+00,  4.7308e+00,\n",
       "          -5.6066e+00, -2.0731e+01, -2.0477e+01, -1.9745e+01, -2.0288e+01],\n",
       "         [-2.0119e+01, -2.1027e+01, -2.0624e+01, -2.1367e+01, -2.2992e+01,\n",
       "          -7.2403e-01, -1.0803e+00,  1.0500e-01, -1.5964e+00, -2.2548e+00,\n",
       "           2.8500e-01, -1.0511e+00,  3.9716e-02,  3.7723e+00, -2.5312e+00,\n",
       "           2.3228e-01, -9.1369e-01, -5.8183e-01, -4.7033e-01, -1.4922e+00,\n",
       "          -1.4813e+00, -8.5359e-01, -2.0409e+00,  1.1805e+00,  6.4085e+00,\n",
       "          -6.3831e+00, -2.1070e+01, -2.0511e+01, -2.0367e+01, -2.0250e+01],\n",
       "         [-2.0845e+01, -2.1030e+01, -2.0904e+01, -2.0538e+01, -2.2470e+01,\n",
       "          -3.4865e-01, -7.7006e-01,  6.4181e-01, -1.5215e+00, -2.1297e+00,\n",
       "           7.3375e-01, -7.7118e-01,  5.2100e-01,  4.2977e+00, -2.7579e+00,\n",
       "           7.3458e-01,  4.5231e-02, -4.6088e-01,  2.8464e-01, -1.7713e+00,\n",
       "          -1.0256e+00, -3.5819e-01, -1.7903e+00,  7.8434e-01,  3.9020e+00,\n",
       "          -5.8730e+00, -2.0613e+01, -2.0840e+01, -2.0357e+01, -2.0624e+01]]],\n",
       "       device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load protbert model\n",
    "from transformers import BertForMaskedLM, BertTokenizer, pipeline\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "protbert_model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\")\n",
    "if GPU:\n",
    "    protbert_model.cuda()\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "x_aminos = re.compile(\"[UZOB]\")\n",
    "\n",
    "def prepare_input(seq, **tokenizer_args):\n",
    "    seq = x_aminos.sub(\"X\", ' '.join(seq))\n",
    "    input_ids = tokenizer.encode(seq, add_special_tokens=True, **tokenizer_args)\n",
    "    return input_ids\n",
    "\n",
    "def get_embeddings(seq, **tokenizer_args):\n",
    "    input_ids = prepare_input(seq, **tokenizer_args)\n",
    "    input_ids = torch.tensor([input_ids], device='cuda' if GPU else 'cpu')\n",
    "    with torch.no_grad():\n",
    "        return protbert_model(input_ids)\n",
    "\n",
    "eg_input = get_embeddings(\"RRWWRRRRW\")\n",
    "eg_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61185c77-fda0-4d24-b105-8f36aacf238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ProteinSequenceDataset(Dataset):\n",
    "    def __init__(self, df, sequence_col='sequence', label_col='family_id', max_len=100):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.sequence_col = sequence_col\n",
    "        self.label_col = label_col\n",
    "        self.max_len = max_len\n",
    "        self._label_translator = {l: torch.tensor(i, device='cuda' if GPU else 'cpu') for i, l in enumerate(sorted(df[label_col].unique()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq, label = row[self.sequence_col], row[self.label_col]\n",
    "        # Convert to tensor\n",
    "        seq = prepare_input(seq[:self.max_len-10], padding='max_length', max_length=self.max_len)\n",
    "        seq = torch.tensor(seq, device='cuda' if GPU else 'cpu')\n",
    "        label = self._label_translator[label]\n",
    "        return seq, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15141a59-5d42-4c64-9e38-3ac9ef56a725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Methyltransf_25', 0),\n",
       " ('LRR_1', 1),\n",
       " ('Acetyltransf_7', 2),\n",
       " ('His_kinase', 3),\n",
       " ('Bac_transf', 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Convert train and test to same top n families\n",
    "top_families = train_df['family_id'].value_counts()[:NUM_CLASSES]\n",
    "# Convert to numbers\n",
    "fam2id = {fam: i for i, fam in enumerate(top_families.index)}\n",
    "list(fam2id.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d8c6e5-3db4-4d27-954e-510a12a12b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 647,248/1,086,741 = 59.558625% of rows due to nan famid num.\n",
      "Removing 71,793/126,171 = 56.901348% of rows due to nan famid num.\n",
      "Removing 71,793/126,171 = 56.901348% of rows due to nan famid num.\n"
     ]
    }
   ],
   "source": [
    "def add_and_filter_family_id(df):\n",
    "    df['family_code'] = df['family_id'].apply(lambda x: fam2id.get(x, np.nan))\n",
    "    logger.info(f'Removing {df[\"family_code\"].isna().sum():,}/{len(df):,} = {df[\"family_code\"].isna().mean()*100:,.6f}% of rows due to nan famid num.')\n",
    "    return df.dropna(subset='family_code').reset_index(drop=True)\n",
    "\n",
    "train_df = add_and_filter_family_id(train_df)\n",
    "test_df = add_and_filter_family_id(test_df)\n",
    "val_df = add_and_filter_family_id(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7de3d469-98c2-47bb-b293-83b96ba054bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((439493, 6), (54378, 6), (54378, 6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc; gc.collect()\n",
    "train_df.shape, test_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a78873-120b-465c-b442-3eacc8ddcd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2, 14, 16,  ...,  0,  0,  0],\n",
       "         [ 2,  6,  5,  ...,  0,  0,  0],\n",
       "         [ 2,  8, 15,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 2, 14, 12,  ...,  0,  0,  0],\n",
       "         [ 2,  5, 19,  ...,  0,  0,  0],\n",
       "         [ 2, 16, 10,  ...,  0,  0,  0]], device='cuda:0'),\n",
       " tensor([433, 289, 117, 749, 215,  85,  59,  31,  63, 185, 494,  46, 986, 298,\n",
       "         383, 107, 865, 314, 164, 441, 337, 199, 413, 348, 320, 367, 688, 223,\n",
       "         618, 392, 541, 906,  65, 454, 831, 423, 229, 259, 465, 468,  14,  43,\n",
       "          11, 178,   8,  81, 417,  33,  55, 489, 721, 962, 875, 601, 174, 350,\n",
       "         747, 161, 257, 367,  35, 277, 228,  64], device='cuda:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ProteinSequenceDataset(train_df, label_col='family_code', max_len=MAX_SEQ_LENGTH)\n",
    "test_dataset  = ProteinSequenceDataset(test_df, label_col='family_code', max_len=MAX_SEQ_LENGTH)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_features, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b661f4-58ff-4fc6-bf3b-f6ddb1291890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer models from tutorial https://n8henrie.com/2021/08/writing-a-transformer-classifier-in-pytorch/\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(vocab_size, d_model)\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Text classifier based on a pytorch TransformerEncoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings,\n",
    "        vocab_size=30,\n",
    "        embedding_size=1024,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        num_layers=6,\n",
    "        num_labels=2,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        classifier_dropout=0.1,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        d_model = embedding_size\n",
    "        assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
    "\n",
    "        self.emb = embeddings\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=d_model,\n",
    "            dropout=dropout,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "        self.d_model = d_model\n",
    "        self._agg_type = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            embeds = self.emb(x)\n",
    "            # x = embeds[:,-10:,:] # Only need last ten\n",
    "            x = embeds\n",
    "        # x = self.emb(x) * math.sqrt(self.d_model)\n",
    "        # x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        if self._agg_type == 0:\n",
    "            x = x[:, -1, :]\n",
    "        else:\n",
    "            x = x.mean(1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8daf6b2c-ca01-4ea8-a85f-16da306e909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = Net(\n",
    "    protbert_model.bert.embeddings,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    nhead=8,  # the number of heads in the multiheadattention models\n",
    "    dim_feedforward=50,  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    num_layers=6,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    dropout=0.2,\n",
    "    classifier_dropout=0.2,\n",
    ").to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    tf_model.parameters(), lr=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29f2d969-34b8-4b00-9b04-6908554c04e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9b34e7f-27fe-4fb0-94cf-1635e8d93a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, log_n=100):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_count = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = tf_model(inputs)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "\n",
    "        correct = predictions.argmax(axis=1) == labels\n",
    "        acc = correct.sum().item() / correct.size(0)\n",
    "        epoch_correct += correct.sum().item()\n",
    "        epoch_count += correct.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % log_n == log_n-1:\n",
    "            logger.info(f'{epoch=} {idx:,}/{len(train_dataloader):,} {epoch_loss=:,.6f}/{epoch_count=:,} = {epoch_loss/epoch_count:,.6f} {epoch_correct:,}/{epoch_count:,} = {100*epoch_correct/epoch_count:,.6f}%')\n",
    "\n",
    "    logger.info(f\"{epoch=} {epoch_loss=}\")\n",
    "    logger.info(f\"{epoch=} accuracy: {epoch_correct / epoch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf0af25e-fc30-41de-b483-cb9a1a1f3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(epoch, log_n=100):\n",
    "    with torch.no_grad():\n",
    "        test_epoch_loss = 0\n",
    "        test_epoch_correct = 0\n",
    "        test_epoch_count = 0\n",
    "\n",
    "        for idx, (inputs, labels) in enumerate(test_dataloader):\n",
    "            predictions = tf_model(inputs)\n",
    "            test_loss = loss_fn(predictions, labels)\n",
    "\n",
    "            correct = predictions.argmax(axis=1) == labels\n",
    "            acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "            test_epoch_correct += correct.sum().item()\n",
    "            test_epoch_count += correct.size(0)\n",
    "            test_epoch_loss += loss.item()\n",
    "            \n",
    "            if idx % log_n == log_n-1:\n",
    "                logger.info(f'{epoch=} {idx:,}/{len(test_dataloader):,} {test_epoch_loss=:,.6f}/{test_epoch_count=:,} = {test_epoch_loss/test_epoch_count:,.6f} {test_epoch_correct:,}/{test_epoch_count:,} = {100*test_epoch_correct/test_epoch_count:,.6f}%')\n",
    "\n",
    "    logger.info(f\"{epoch=} {test_epoch_loss=}\")\n",
    "    logger.info(f\"test {epoch=} accuracy: {test_epoch_correct=:,}/{test_epoch_count:,} = {100 * test_epoch_correct / test_epoch_count:,.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55360f7f-cb55-4b3f-825b-3c4013fd7cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 18:04:53 [info     ] epoch=0\n",
      "2023-01-21 18:05:26 [info     ] epoch=0 99/6,868 epoch_loss=565.347676/epoch_count=6,400 = 0.088336 156/6,400 = 2.437500%\n",
      "2023-01-21 18:05:59 [info     ] epoch=0 199/6,868 epoch_loss=1,128.954273/epoch_count=12,800 = 0.088200 316/12,800 = 2.468750%\n",
      "2023-01-21 18:06:33 [info     ] epoch=0 299/6,868 epoch_loss=1,692.231605/epoch_count=19,200 = 0.088137 475/19,200 = 2.473958%\n",
      "2023-01-21 18:07:06 [info     ] epoch=0 399/6,868 epoch_loss=2,253.782454/epoch_count=25,600 = 0.088038 624/25,600 = 2.437500%\n",
      "2023-01-21 18:07:40 [info     ] epoch=0 499/6,868 epoch_loss=2,813.347023/epoch_count=32,000 = 0.087917 785/32,000 = 2.453125%\n",
      "2023-01-21 18:08:14 [info     ] epoch=0 599/6,868 epoch_loss=3,373.556302/epoch_count=38,400 = 0.087853 945/38,400 = 2.460938%\n",
      "2023-01-21 18:08:47 [info     ] epoch=0 699/6,868 epoch_loss=3,932.364497/epoch_count=44,800 = 0.087776 1,097/44,800 = 2.448661%\n",
      "2023-01-21 18:09:21 [info     ] epoch=0 799/6,868 epoch_loss=4,488.368099/epoch_count=51,200 = 0.087663 1,267/51,200 = 2.474609%\n",
      "2023-01-21 18:09:55 [info     ] epoch=0 899/6,868 epoch_loss=5,044.779979/epoch_count=57,600 = 0.087583 1,438/57,600 = 2.496528%\n",
      "2023-01-21 18:10:29 [info     ] epoch=0 999/6,868 epoch_loss=5,601.445614/epoch_count=64,000 = 0.087523 1,585/64,000 = 2.476562%\n",
      "2023-01-21 18:11:02 [info     ] epoch=0 1,099/6,868 epoch_loss=6,154.667025/epoch_count=70,400 = 0.087424 1,744/70,400 = 2.477273%\n",
      "2023-01-21 18:11:36 [info     ] epoch=0 1,199/6,868 epoch_loss=6,708.864788/epoch_count=76,800 = 0.087355 1,906/76,800 = 2.481771%\n",
      "2023-01-21 18:12:10 [info     ] epoch=0 1,299/6,868 epoch_loss=7,261.316268/epoch_count=83,200 = 0.087275 2,085/83,200 = 2.506010%\n",
      "2023-01-21 18:12:44 [info     ] epoch=0 1,399/6,868 epoch_loss=7,811.357190/epoch_count=89,600 = 0.087180 2,293/89,600 = 2.559152%\n",
      "2023-01-21 18:13:17 [info     ] epoch=0 1,499/6,868 epoch_loss=8,361.454453/epoch_count=96,000 = 0.087098 2,465/96,000 = 2.567708%\n",
      "2023-01-21 18:13:51 [info     ] epoch=0 1,599/6,868 epoch_loss=8,909.658042/epoch_count=102,400 = 0.087008 2,653/102,400 = 2.590820%\n",
      "2023-01-21 18:14:25 [info     ] epoch=0 1,699/6,868 epoch_loss=9,456.840456/epoch_count=108,800 = 0.086919 2,829/108,800 = 2.600184%\n",
      "2023-01-21 18:14:58 [info     ] epoch=0 1,799/6,868 epoch_loss=10,003.257294/epoch_count=115,200 = 0.086834 2,994/115,200 = 2.598958%\n",
      "2023-01-21 18:15:32 [info     ] epoch=0 1,899/6,868 epoch_loss=10,548.537422/epoch_count=121,600 = 0.086748 3,172/121,600 = 2.608553%\n",
      "2023-01-21 18:16:06 [info     ] epoch=0 1,999/6,868 epoch_loss=11,093.158399/epoch_count=128,000 = 0.086665 3,336/128,000 = 2.606250%\n",
      "2023-01-21 18:16:39 [info     ] epoch=0 2,099/6,868 epoch_loss=11,637.197263/epoch_count=134,400 = 0.086586 3,513/134,400 = 2.613839%\n",
      "2023-01-21 18:17:13 [info     ] epoch=0 2,199/6,868 epoch_loss=12,180.843170/epoch_count=140,800 = 0.086512 3,715/140,800 = 2.638494%\n",
      "2023-01-21 18:17:47 [info     ] epoch=0 2,299/6,868 epoch_loss=12,722.633683/epoch_count=147,200 = 0.086431 3,895/147,200 = 2.646060%\n",
      "2023-01-21 18:18:20 [info     ] epoch=0 2,399/6,868 epoch_loss=13,264.279289/epoch_count=153,600 = 0.086356 4,088/153,600 = 2.661458%\n",
      "2023-01-21 18:18:54 [info     ] epoch=0 2,499/6,868 epoch_loss=13,803.375613/epoch_count=160,000 = 0.086271 4,275/160,000 = 2.671875%\n",
      "2023-01-21 18:19:28 [info     ] epoch=0 2,599/6,868 epoch_loss=14,343.193116/epoch_count=166,400 = 0.086197 4,447/166,400 = 2.672476%\n",
      "2023-01-21 18:20:01 [info     ] epoch=0 2,699/6,868 epoch_loss=14,880.757818/epoch_count=172,800 = 0.086115 4,658/172,800 = 2.695602%\n",
      "2023-01-21 18:20:35 [info     ] epoch=0 2,799/6,868 epoch_loss=15,417.853773/epoch_count=179,200 = 0.086037 4,859/179,200 = 2.711496%\n",
      "2023-01-21 18:21:09 [info     ] epoch=0 2,899/6,868 epoch_loss=15,953.639973/epoch_count=185,600 = 0.085957 5,062/185,600 = 2.727371%\n",
      "2023-01-21 18:21:42 [info     ] epoch=0 2,999/6,868 epoch_loss=16,487.649595/epoch_count=192,000 = 0.085873 5,287/192,000 = 2.753646%\n",
      "2023-01-21 18:22:16 [info     ] epoch=0 3,099/6,868 epoch_loss=17,020.855605/epoch_count=198,400 = 0.085791 5,513/198,400 = 2.778730%\n",
      "2023-01-21 18:22:50 [info     ] epoch=0 3,199/6,868 epoch_loss=17,554.876737/epoch_count=204,800 = 0.085717 5,759/204,800 = 2.812012%\n",
      "2023-01-21 18:23:23 [info     ] epoch=0 3,299/6,868 epoch_loss=18,087.507069/epoch_count=211,200 = 0.085642 5,967/211,200 = 2.825284%\n",
      "2023-01-21 18:23:57 [info     ] epoch=0 3,399/6,868 epoch_loss=18,618.978106/epoch_count=217,600 = 0.085565 6,177/217,600 = 2.838695%\n",
      "2023-01-21 18:24:31 [info     ] epoch=0 3,499/6,868 epoch_loss=19,147.763319/epoch_count=224,000 = 0.085481 6,387/224,000 = 2.851339%\n",
      "2023-01-21 18:25:04 [info     ] epoch=0 3,599/6,868 epoch_loss=19,676.979521/epoch_count=230,400 = 0.085404 6,643/230,400 = 2.883247%\n",
      "2023-01-21 18:25:38 [info     ] epoch=0 3,699/6,868 epoch_loss=20,204.579484/epoch_count=236,800 = 0.085323 6,890/236,800 = 2.909628%\n",
      "2023-01-21 18:26:12 [info     ] epoch=0 3,799/6,868 epoch_loss=20,731.109782/epoch_count=243,200 = 0.085243 7,138/243,200 = 2.935033%\n",
      "2023-01-21 18:26:45 [info     ] epoch=0 3,899/6,868 epoch_loss=21,258.051365/epoch_count=249,600 = 0.085168 7,378/249,600 = 2.955929%\n",
      "2023-01-21 18:27:19 [info     ] epoch=0 3,999/6,868 epoch_loss=21,783.794547/epoch_count=256,000 = 0.085093 7,642/256,000 = 2.985156%\n",
      "2023-01-21 18:27:53 [info     ] epoch=0 4,099/6,868 epoch_loss=22,308.145490/epoch_count=262,400 = 0.085016 7,896/262,400 = 3.009146%\n",
      "2023-01-21 18:28:27 [info     ] epoch=0 4,199/6,868 epoch_loss=22,830.795520/epoch_count=268,800 = 0.084936 8,179/268,800 = 3.042783%\n",
      "2023-01-21 18:29:00 [info     ] epoch=0 4,299/6,868 epoch_loss=23,352.091729/epoch_count=275,200 = 0.084855 8,459/275,200 = 3.073765%\n",
      "2023-01-21 18:29:34 [info     ] epoch=0 4,399/6,868 epoch_loss=23,872.728964/epoch_count=281,600 = 0.084775 8,728/281,600 = 3.099432%\n",
      "2023-01-21 18:30:07 [info     ] epoch=0 4,499/6,868 epoch_loss=24,393.395745/epoch_count=288,000 = 0.084699 8,993/288,000 = 3.122569%\n",
      "2023-01-21 18:30:41 [info     ] epoch=0 4,599/6,868 epoch_loss=24,912.154404/epoch_count=294,400 = 0.084620 9,252/294,400 = 3.142663%\n",
      "2023-01-21 18:31:15 [info     ] epoch=0 4,699/6,868 epoch_loss=25,431.796698/epoch_count=300,800 = 0.084547 9,545/300,800 = 3.173205%\n",
      "2023-01-21 18:31:49 [info     ] epoch=0 4,799/6,868 epoch_loss=25,947.586507/epoch_count=307,200 = 0.084465 9,855/307,200 = 3.208008%\n",
      "2023-01-21 18:32:22 [info     ] epoch=0 4,899/6,868 epoch_loss=26,462.782400/epoch_count=313,600 = 0.084384 10,171/313,600 = 3.243304%\n",
      "2023-01-21 18:32:56 [info     ] epoch=0 4,999/6,868 epoch_loss=26,977.699365/epoch_count=320,000 = 0.084305 10,472/320,000 = 3.272500%\n",
      "2023-01-21 18:33:30 [info     ] epoch=0 5,099/6,868 epoch_loss=27,491.672073/epoch_count=326,400 = 0.084227 10,776/326,400 = 3.301471%\n",
      "2023-01-21 18:34:03 [info     ] epoch=0 5,199/6,868 epoch_loss=28,005.145705/epoch_count=332,800 = 0.084150 11,093/332,800 = 3.333233%\n",
      "2023-01-21 18:34:37 [info     ] epoch=0 5,299/6,868 epoch_loss=28,516.722130/epoch_count=339,200 = 0.084071 11,420/339,200 = 3.366745%\n",
      "2023-01-21 18:35:11 [info     ] epoch=0 5,399/6,868 epoch_loss=29,027.571859/epoch_count=345,600 = 0.083992 11,786/345,600 = 3.410301%\n",
      "2023-01-21 18:35:44 [info     ] epoch=0 5,499/6,868 epoch_loss=29,539.608591/epoch_count=352,000 = 0.083919 12,112/352,000 = 3.440909%\n",
      "2023-01-21 18:36:18 [info     ] epoch=0 5,599/6,868 epoch_loss=30,049.062709/epoch_count=358,400 = 0.083842 12,448/358,400 = 3.473214%\n",
      "2023-01-21 18:36:52 [info     ] epoch=0 5,699/6,868 epoch_loss=30,558.677554/epoch_count=364,800 = 0.083768 12,789/364,800 = 3.505757%\n",
      "2023-01-21 18:37:25 [info     ] epoch=0 5,799/6,868 epoch_loss=31,065.327684/epoch_count=371,200 = 0.083689 13,147/371,200 = 3.541756%\n",
      "2023-01-21 18:37:59 [info     ] epoch=0 5,899/6,868 epoch_loss=31,571.063576/epoch_count=377,600 = 0.083610 13,512/377,600 = 3.578390%\n",
      "2023-01-21 18:38:33 [info     ] epoch=0 5,999/6,868 epoch_loss=32,076.322323/epoch_count=384,000 = 0.083532 13,882/384,000 = 3.615104%\n",
      "2023-01-21 18:39:06 [info     ] epoch=0 6,099/6,868 epoch_loss=32,581.879406/epoch_count=390,400 = 0.083458 14,230/390,400 = 3.644980%\n",
      "2023-01-21 18:39:40 [info     ] epoch=0 6,199/6,868 epoch_loss=33,084.448093/epoch_count=396,800 = 0.083378 14,637/396,800 = 3.688760%\n",
      "2023-01-21 18:40:14 [info     ] epoch=0 6,299/6,868 epoch_loss=33,585.972829/epoch_count=403,200 = 0.083299 15,029/403,200 = 3.727431%\n",
      "2023-01-21 18:40:47 [info     ] epoch=0 6,399/6,868 epoch_loss=34,087.555357/epoch_count=409,600 = 0.083222 15,414/409,600 = 3.763184%\n",
      "2023-01-21 18:41:21 [info     ] epoch=0 6,499/6,868 epoch_loss=34,586.955379/epoch_count=416,000 = 0.083142 15,848/416,000 = 3.809615%\n",
      "2023-01-21 18:41:54 [info     ] epoch=0 6,599/6,868 epoch_loss=35,085.185564/epoch_count=422,400 = 0.083062 16,271/422,400 = 3.852036%\n",
      "2023-01-21 18:42:28 [info     ] epoch=0 6,699/6,868 epoch_loss=35,584.090926/epoch_count=428,800 = 0.082985 16,667/428,800 = 3.886894%\n",
      "2023-01-21 18:43:02 [info     ] epoch=0 6,799/6,868 epoch_loss=36,080.698788/epoch_count=435,200 = 0.082906 17,105/435,200 = 3.930377%\n",
      "2023-01-21 18:43:24 [info     ] epoch=0 epoch_loss=36418.03007221222\n",
      "2023-01-21 18:43:24 [info     ] epoch=0 accuracy: 0.039529639834991685\n",
      "2023-01-21 18:43:40 [info     ] epoch=0 99/850 test_epoch_loss=577.238035/test_epoch_count=6,400 = 0.090193 444/6,400 = 6.937500%\n",
      "2023-01-21 18:43:56 [info     ] epoch=0 199/850 test_epoch_loss=1,154.476070/test_epoch_count=12,800 = 0.090193 878/12,800 = 6.859375%\n",
      "2023-01-21 18:44:11 [info     ] epoch=0 299/850 test_epoch_loss=1,731.714106/test_epoch_count=19,200 = 0.090193 1,316/19,200 = 6.854167%\n",
      "2023-01-21 18:44:27 [info     ] epoch=0 399/850 test_epoch_loss=2,308.952141/test_epoch_count=25,600 = 0.090193 1,762/25,600 = 6.882812%\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# with torch.autocast(device_type='cuda' if GPU else 'cpu'):\n",
    "for epoch in range(EPOCHS):\n",
    "    logger.info(f\"{epoch=}\")\n",
    "    train_one_epoch(epoch)\n",
    "    test_one_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76fc4fd8-e0fd-4808-b057-140788504bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 18:02:38 [info     ] epoch=0 29/850 test_epoch_loss=173.171411/test_epoch_count=1,920 = 0.090193 50/1,920 = 2.604167%\n",
      "2023-01-21 18:02:43 [info     ] epoch=0 59/850 test_epoch_loss=346.342821/test_epoch_count=3,840 = 0.090193 103/3,840 = 2.682292%\n",
      "2023-01-21 18:02:47 [info     ] epoch=0 89/850 test_epoch_loss=519.514232/test_epoch_count=5,760 = 0.090193 157/5,760 = 2.725694%\n",
      "2023-01-21 18:02:52 [info     ] epoch=0 119/850 test_epoch_loss=692.685642/test_epoch_count=7,680 = 0.090193 207/7,680 = 2.695312%\n",
      "2023-01-21 18:02:57 [info     ] epoch=0 149/850 test_epoch_loss=865.857053/test_epoch_count=9,600 = 0.090193 256/9,600 = 2.666667%\n",
      "2023-01-21 18:03:01 [info     ] epoch=0 179/850 test_epoch_loss=1,039.028463/test_epoch_count=11,520 = 0.090193 304/11,520 = 2.638889%\n",
      "2023-01-21 18:03:06 [info     ] epoch=0 209/850 test_epoch_loss=1,212.199874/test_epoch_count=13,440 = 0.090193 353/13,440 = 2.626488%\n",
      "2023-01-21 18:03:10 [info     ] epoch=0 239/850 test_epoch_loss=1,385.371284/test_epoch_count=15,360 = 0.090193 405/15,360 = 2.636719%\n",
      "2023-01-21 18:03:15 [info     ] epoch=0 269/850 test_epoch_loss=1,558.542695/test_epoch_count=17,280 = 0.090193 443/17,280 = 2.563657%\n",
      "2023-01-21 18:03:20 [info     ] epoch=0 299/850 test_epoch_loss=1,731.714106/test_epoch_count=19,200 = 0.090193 505/19,200 = 2.630208%\n",
      "2023-01-21 18:03:24 [info     ] epoch=0 329/850 test_epoch_loss=1,904.885516/test_epoch_count=21,120 = 0.090193 556/21,120 = 2.632576%\n",
      "2023-01-21 18:03:29 [info     ] epoch=0 359/850 test_epoch_loss=2,078.056927/test_epoch_count=23,040 = 0.090193 606/23,040 = 2.630208%\n",
      "2023-01-21 18:03:34 [info     ] epoch=0 389/850 test_epoch_loss=2,251.228337/test_epoch_count=24,960 = 0.090193 653/24,960 = 2.616186%\n",
      "2023-01-21 18:03:38 [info     ] epoch=0 419/850 test_epoch_loss=2,424.399748/test_epoch_count=26,880 = 0.090193 701/26,880 = 2.607887%\n",
      "2023-01-21 18:03:43 [info     ] epoch=0 449/850 test_epoch_loss=2,597.571158/test_epoch_count=28,800 = 0.090193 747/28,800 = 2.593750%\n",
      "2023-01-21 18:03:47 [info     ] epoch=0 479/850 test_epoch_loss=2,770.742569/test_epoch_count=30,720 = 0.090193 788/30,720 = 2.565104%\n",
      "2023-01-21 18:03:52 [info     ] epoch=0 509/850 test_epoch_loss=2,943.913980/test_epoch_count=32,640 = 0.090193 835/32,640 = 2.558211%\n",
      "2023-01-21 18:03:57 [info     ] epoch=0 539/850 test_epoch_loss=3,117.085390/test_epoch_count=34,560 = 0.090193 884/34,560 = 2.557870%\n",
      "2023-01-21 18:04:01 [info     ] epoch=0 569/850 test_epoch_loss=3,290.256801/test_epoch_count=36,480 = 0.090193 946/36,480 = 2.593202%\n",
      "2023-01-21 18:04:06 [info     ] epoch=0 599/850 test_epoch_loss=3,463.428211/test_epoch_count=38,400 = 0.090193 984/38,400 = 2.562500%\n",
      "2023-01-21 18:04:11 [info     ] epoch=0 629/850 test_epoch_loss=3,636.599622/test_epoch_count=40,320 = 0.090193 1,036/40,320 = 2.569444%\n",
      "2023-01-21 18:04:15 [info     ] epoch=0 659/850 test_epoch_loss=3,809.771032/test_epoch_count=42,240 = 0.090193 1,079/42,240 = 2.554451%\n",
      "2023-01-21 18:04:20 [info     ] epoch=0 689/850 test_epoch_loss=3,982.942443/test_epoch_count=44,160 = 0.090193 1,126/44,160 = 2.549819%\n",
      "2023-01-21 18:04:25 [info     ] epoch=0 719/850 test_epoch_loss=4,156.113853/test_epoch_count=46,080 = 0.090193 1,163/46,080 = 2.523872%\n",
      "2023-01-21 18:04:29 [info     ] epoch=0 749/850 test_epoch_loss=4,329.285264/test_epoch_count=48,000 = 0.090193 1,212/48,000 = 2.525000%\n",
      "2023-01-21 18:04:34 [info     ] epoch=0 779/850 test_epoch_loss=4,502.456675/test_epoch_count=49,920 = 0.090193 1,259/49,920 = 2.522035%\n",
      "2023-01-21 18:04:39 [info     ] epoch=0 809/850 test_epoch_loss=4,675.628085/test_epoch_count=51,840 = 0.090193 1,311/51,840 = 2.528935%\n",
      "2023-01-21 18:04:43 [info     ] epoch=0 839/850 test_epoch_loss=4,848.799496/test_epoch_count=53,760 = 0.090193 1,357/53,760 = 2.524182%\n",
      "2023-01-21 18:04:45 [info     ] epoch=0 test_epoch_loss=4906.523299217224\n",
      "2023-01-21 18:04:45 [info     ] test epoch=0 accuracy: test_epoch_correct=1,371/54,378 = 2.521240%\n"
     ]
    }
   ],
   "source": [
    "test_one_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f3caeee-9f72-4502-873b-929d057a6a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 19,  9,  ...,  0,  0,  0],\n",
      "        [ 2,  6,  6,  ...,  0,  0,  0],\n",
      "        [ 2, 16,  8,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 2, 20, 12,  ...,  0,  0,  0],\n",
      "        [ 2, 11, 21,  ...,  0,  0,  0],\n",
      "        [ 2,  6, 23,  ...,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[-0.0440, -5.3381, -0.9999,  ...,  1.2580,  1.1968, -0.1832],\n",
      "        [-0.5925, -5.6488, -1.5915,  ...,  1.4442,  1.2875,  0.3802],\n",
      "        [ 3.4140,  0.9251,  3.1872,  ..., -1.3907, -0.8055, -2.7464],\n",
      "        ...,\n",
      "        [ 2.4615,  2.9081,  2.8831,  ..., -1.6114, -1.2116, -2.2700],\n",
      "        [ 3.4396,  0.9174,  3.2128,  ..., -1.4100, -0.7952, -2.7663],\n",
      "        [-3.4412, -1.6198, -3.4318,  ...,  1.0870,  0.4103,  2.9897]],\n",
      "       device='cuda:0')\n",
      "tensor(5.6799, device='cuda:0')\n",
      "tensor([ 4,  4,  0, 47, 47,  0, 47,  7,  0,  3,  4, 12,  0,  2, 16, 47,  1,  0,\n",
      "         0,  0,  0, 47,  0,  3,  4,  0,  4,  0,  3,  0,  0,  0,  2, 12,  4,  0,\n",
      "         0,  7,  0,  0, 47,  0,  0, 47, 47,  2,  0,  2, 47, 47,  0, 47,  0,  0,\n",
      "         0,  0,  0, 12,  1, 47, 47, 38,  0, 47], device='cuda:0')\n",
      "tensor([556, 412, 910, 716, 834, 756, 486, 303, 185, 934,  24, 232, 516, 380,\n",
      "         59, 577, 176, 645, 308, 375,  68, 891, 286,  35, 727,   2, 373, 976,\n",
      "        550, 364,   0, 100, 913, 921,   4, 121, 371, 445, 107,  88, 691, 762,\n",
      "          2, 450, 644, 702, 288, 116, 150, 381, 593, 985, 300,   3, 285, 114,\n",
      "        246,  95, 351, 152, 148, 140, 387, 388], device='cuda:0')\n",
      "tensor([[ 2, 17, 14,  ...,  0,  0,  0],\n",
      "        [ 2,  8,  9,  ...,  0,  0,  0],\n",
      "        [ 2,  8, 11,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 2, 10, 19,  ...,  0,  0,  0],\n",
      "        [ 2, 14,  8,  ...,  0,  0,  0],\n",
      "        [ 2,  5, 19,  ...,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[-2.1035, -3.6559, -2.5492,  ...,  1.3076,  0.8787,  1.7513],\n",
      "        [ 2.5221,  2.7949,  2.8812,  ..., -1.6095, -1.1952, -2.3280],\n",
      "        [-3.3094, -1.7706, -3.3902,  ...,  1.0708,  0.4321,  2.9613],\n",
      "        ...,\n",
      "        [-0.4641, -5.0898, -1.2701,  ...,  1.3190,  1.1786,  0.1023],\n",
      "        [ 3.9131, -0.6407,  3.2264,  ..., -1.1441, -0.4575, -2.9701],\n",
      "        [-3.1041, -2.1983, -3.2975,  ...,  1.1143,  0.5045,  2.8489]],\n",
      "       device='cuda:0')\n",
      "tensor(5.5846, device='cuda:0')\n",
      "tensor([16, 38, 47,  1, 47, 47,  4,  0,  0,  0,  4,  0,  3, 47, 47,  0,  0,  3,\n",
      "         1,  7,  0,  1,  7,  0,  0, 47, 47,  0,  0,  1,  0, 12,  1,  0,  0,  0,\n",
      "         4,  0, 47,  0,  0,  0, 47,  7,  4, 47,  7,  0,  0,  7,  0,  4,  0,  1,\n",
      "        47, 47,  0,  0, 16,  0,  0,  4,  0, 47], device='cuda:0')\n",
      "tensor([214, 468,  81, 839, 171,  69, 236, 459, 274, 293,  78, 154,  17, 670,\n",
      "        148, 285, 504, 584, 102, 558,  57, 538,   7, 500,  89,  69, 111, 143,\n",
      "        774, 441, 573, 252, 820, 490, 682,  22, 981,  15, 716, 115, 308, 557,\n",
      "        144,  20, 754,  83, 125, 583,   3, 705, 664, 589,  94, 366, 306, 310,\n",
      "        824,  43, 904,   6,   2, 754, 530, 706], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Verify its not just giving the same label or getting lucky\n",
    "with torch.no_grad():\n",
    "    test_epoch_loss = 0\n",
    "    test_epoch_correct = 0\n",
    "    test_epoch_count = 0\n",
    "    for idx, (inputs, labels) in enumerate(test_dataloader):\n",
    "        predictions = tf_model(inputs)\n",
    "        test_loss = loss_fn(predictions, labels)\n",
    "        print(inputs)\n",
    "        print(predictions)\n",
    "        print(test_loss)\n",
    "        print(predictions.argmax(axis=1))\n",
    "        print(labels)\n",
    "        if idx * BATCH_SIZE > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29a6e8-0591-4a91-994e-a9d07d9821c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549c42d-63f6-4846-b999-7cd451151c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
