{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d8db45-849d-41e6-9240-767f67d957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "NUM_CLASSES=1_000\n",
    "BATCH_SIZE = 2**6\n",
    "MAX_SEQ_LENGTH = 256\n",
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0517f53-d48c-4f94-a758-809124c81f45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext in ./.local/lib/python3.8/site-packages (0.14.1)\n",
      "Requirement already satisfied: pyarrow in ./.local/lib/python3.8/site-packages (10.0.1)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.8/site-packages (4.25.1)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from torchtext) (4.64.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from torchtext) (1.23.4)\n",
      "Requirement already satisfied: torch==1.13.1 in ./.local/lib/python3.8/site-packages (from torchtext) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.local/lib/python3.8/site-packages (from torch==1.13.1->torchtext) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.local/lib/python3.8/site-packages (from torch==1.13.1->torchtext) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.8/site-packages (from torch==1.13.1->torchtext) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.local/lib/python3.8/site-packages (from torch==1.13.1->torchtext) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.local/lib/python3.8/site-packages (from torch==1.13.1->torchtext) (11.7.99)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchtext) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchtext) (45.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.local/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in ./.local/lib/python3.8/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchtext) (1.25.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.local/lib/python3.8/site-packages (from requests->torchtext) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchtext) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchtext) (2.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext pyarrow transformers structlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97dfadde-6359-4359-b514-fd30c16b3bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import gc\n",
    "import structlog\n",
    "\n",
    "GPU = True\n",
    "device = torch.device(\"cuda\" if GPU else \"cpu\")\n",
    "data_path = '/home/ubuntu/'\n",
    "logger = structlog.getLogger()\n",
    "logger.info(f\"Getting started with {GPU=} {device=} {data_path=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ac24e0-d77c-42b5-bba7-f7ec70be4a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/ubuntu/\u001b[00m\n",
      "├── \u001b[01;31mparquet_data.zip\u001b[00m\n",
      "├── \u001b[01;34msnap\u001b[00m\n",
      "│   └── \u001b[01;34mnvtop\u001b[00m\n",
      "│       ├── \u001b[01;34m66\u001b[00m\n",
      "│       ├── \u001b[01;34mcommon\u001b[00m\n",
      "│       └── \u001b[01;36mcurrent\u001b[00m -> \u001b[01;34m66\u001b[00m\n",
      "├── test_df.parquet\n",
      "├── train_df.parquet\n",
      "├── try1.ipynb\n",
      "└── val_df.parquet\n",
      "\n",
      "5 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree /home/ubuntu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd25c11f-dcc0-4400-aca8-500e3e58a6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1086741, 5), (126171, 5), (126171, 5))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_parquet(data_path + 'train_df.parquet')\n",
    "test_df = pd.read_parquet(data_path + 'test_df.parquet')\n",
    "val_df = pd.read_parquet(data_path + 'val_df.parquet')\n",
    "train_df.shape, test_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c47a72-5601-443e-b947-3b9bb9ba9e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.13.1+cu117', '0.14.1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1757ff69-2684-4b11-9bf5-9f6a5f8a7f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[-1.8532e+01, -1.9899e+01, -1.9202e+01, -1.9233e+01, -2.1818e+01,\n",
       "          -3.3183e-01, -4.3617e-01, -1.6150e-01, -2.5575e-02, -1.2855e+00,\n",
       "           4.6539e-01, -1.3698e+00,  2.5481e-02,  1.3961e+00, -2.3760e+00,\n",
       "          -2.5161e-01, -6.2316e-01, -9.2688e-01, -8.8103e-01, -1.4678e+00,\n",
       "          -1.6217e+00,  3.2145e+00, -2.0604e+00, -4.5068e-01,  4.5483e-01,\n",
       "          -3.9251e+00, -1.8108e+01, -1.8154e+01, -1.8443e+01, -1.8931e+01],\n",
       "         [-2.1589e+01, -2.1636e+01, -2.1418e+01, -2.1295e+01, -2.2876e+01,\n",
       "          -8.4695e-01, -8.1240e-01,  7.8902e-01, -1.7455e+00, -1.4653e+00,\n",
       "           5.6490e-01, -1.1674e+00,  1.2356e+00,  4.1438e+00, -2.4756e+00,\n",
       "           2.0495e+00, -1.3056e+00,  7.8864e-01,  1.2412e+00, -2.7056e+00,\n",
       "          -1.4608e+00, -3.3685e-01, -2.7666e+00,  2.4256e-01,  3.9161e+00,\n",
       "          -6.9252e+00, -2.1043e+01, -2.1170e+01, -2.0820e+01, -2.1201e+01],\n",
       "         [-2.1355e+01, -2.0936e+01, -2.0803e+01, -1.9996e+01, -2.2508e+01,\n",
       "          -1.0702e+00, -1.2807e+00,  8.0083e-01, -2.5328e+00, -2.0794e+00,\n",
       "          -7.4695e-02, -2.3765e+00, -8.2487e-02,  7.4417e+00, -2.5173e+00,\n",
       "          -2.1427e-01, -4.9526e-01, -8.6128e-01,  4.1846e-01, -2.9880e+00,\n",
       "          -2.2347e+00, -1.0304e+00, -2.0541e+00,  3.1538e-01,  3.7330e+00,\n",
       "          -6.5409e+00, -2.0793e+01, -2.0568e+01, -2.0238e+01, -2.0369e+01],\n",
       "         [-2.0309e+01, -2.1225e+01, -2.0786e+01, -2.1611e+01, -2.2968e+01,\n",
       "          -6.3732e-01, -1.3872e+00,  5.2063e-01, -1.9765e+00, -2.2919e+00,\n",
       "           2.1752e-01, -1.3139e+00, -3.5396e-01,  4.5471e+00, -2.6896e+00,\n",
       "          -4.1946e-01, -7.7500e-01, -9.1249e-01, -4.1802e-01, -1.5727e+00,\n",
       "          -1.8177e+00, -1.3429e+00, -1.4203e+00,  1.6126e+00,  6.4946e+00,\n",
       "          -6.4191e+00, -2.1311e+01, -2.0745e+01, -2.0646e+01, -2.0351e+01],\n",
       "         [-2.0537e+01, -2.1130e+01, -2.0580e+01, -2.1501e+01, -2.3339e+01,\n",
       "          -1.2329e-01, -1.4036e+00, -4.4960e-01, -1.8578e+00, -2.2360e+00,\n",
       "           2.5257e-01, -1.3333e+00,  3.5623e-01,  3.7940e+00, -3.2670e+00,\n",
       "           5.4703e-01, -3.9180e-01, -4.6327e-01,  2.8046e-01, -1.2187e+00,\n",
       "          -1.7014e+00, -4.2954e-01, -2.4735e+00,  5.4795e-01,  7.8722e+00,\n",
       "          -6.0718e+00, -2.0745e+01, -2.0331e+01, -2.0259e+01, -2.0217e+01],\n",
       "         [-2.1681e+01, -2.1723e+01, -2.1198e+01, -2.0857e+01, -2.3232e+01,\n",
       "          -4.7492e-01, -1.7424e+00, -8.1062e-01, -2.1330e+00, -2.0256e+00,\n",
       "           5.8700e-01, -6.6166e-01,  2.3291e+00,  4.6294e+00, -3.7499e+00,\n",
       "           1.4300e+00, -1.6061e+00,  8.1381e-01,  2.5701e+00, -2.5303e+00,\n",
       "          -1.6739e+00,  5.9004e-01, -2.6787e+00, -2.1176e-01,  4.4382e+00,\n",
       "          -6.2795e+00, -2.0782e+01, -2.0877e+01, -2.0428e+01, -2.0842e+01],\n",
       "         [-2.1463e+01, -2.0966e+01, -2.1074e+01, -2.0067e+01, -2.2693e+01,\n",
       "          -6.2267e-01, -7.8163e-01, -9.2801e-02, -2.3375e+00, -1.6929e+00,\n",
       "          -3.6896e-01, -2.4060e+00,  2.4572e-01,  7.2160e+00, -2.4930e+00,\n",
       "           3.9765e-03, -8.6921e-01, -7.7966e-01,  6.9086e-01, -2.8618e+00,\n",
       "          -2.5252e+00, -6.8937e-01, -2.5183e+00, -3.7424e-01,  3.2148e+00,\n",
       "          -6.9142e+00, -2.0618e+01, -2.0634e+01, -2.0460e+01, -2.0323e+01],\n",
       "         [-2.1726e+01, -2.1593e+01, -2.1576e+01, -1.9898e+01, -2.3151e+01,\n",
       "          -5.6528e-01, -1.4556e+00,  3.5838e-01, -2.6066e+00, -2.4335e+00,\n",
       "           3.1791e-01, -2.3757e+00, -7.8839e-01,  6.8987e+00, -2.9696e+00,\n",
       "          -3.6685e-01, -4.6606e-01, -1.2922e+00, -1.1873e-01, -1.9211e+00,\n",
       "          -1.7726e+00, -1.3445e+00, -2.2835e+00,  1.2320e+00,  4.1059e+00,\n",
       "          -6.9427e+00, -2.1245e+01, -2.1350e+01, -2.0804e+01, -2.1145e+01],\n",
       "         [-2.1187e+01, -2.0810e+01, -2.0729e+01, -2.0024e+01, -2.2602e+01,\n",
       "          -1.3754e+00, -1.4733e+00,  6.6327e-01, -2.4095e+00, -2.2636e+00,\n",
       "          -9.5332e-02, -2.3874e+00, -6.3662e-01,  6.8972e+00, -2.7144e+00,\n",
       "          -2.5882e-01, -6.1134e-01, -9.7689e-01, -6.1487e-02, -2.5422e+00,\n",
       "          -1.5595e+00, -1.1254e+00, -2.4075e+00,  1.1938e+00,  4.7308e+00,\n",
       "          -5.6066e+00, -2.0731e+01, -2.0477e+01, -1.9745e+01, -2.0288e+01],\n",
       "         [-2.0119e+01, -2.1027e+01, -2.0624e+01, -2.1367e+01, -2.2992e+01,\n",
       "          -7.2403e-01, -1.0803e+00,  1.0500e-01, -1.5964e+00, -2.2548e+00,\n",
       "           2.8500e-01, -1.0511e+00,  3.9716e-02,  3.7723e+00, -2.5312e+00,\n",
       "           2.3228e-01, -9.1369e-01, -5.8183e-01, -4.7033e-01, -1.4922e+00,\n",
       "          -1.4813e+00, -8.5359e-01, -2.0409e+00,  1.1805e+00,  6.4085e+00,\n",
       "          -6.3831e+00, -2.1070e+01, -2.0511e+01, -2.0367e+01, -2.0250e+01],\n",
       "         [-2.0845e+01, -2.1030e+01, -2.0904e+01, -2.0538e+01, -2.2470e+01,\n",
       "          -3.4865e-01, -7.7006e-01,  6.4181e-01, -1.5215e+00, -2.1297e+00,\n",
       "           7.3375e-01, -7.7118e-01,  5.2100e-01,  4.2977e+00, -2.7579e+00,\n",
       "           7.3458e-01,  4.5231e-02, -4.6088e-01,  2.8464e-01, -1.7713e+00,\n",
       "          -1.0256e+00, -3.5819e-01, -1.7903e+00,  7.8434e-01,  3.9020e+00,\n",
       "          -5.8730e+00, -2.0613e+01, -2.0840e+01, -2.0357e+01, -2.0624e+01]]],\n",
       "       device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load protbert model\n",
    "from transformers import BertForMaskedLM, BertTokenizer, pipeline\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "protbert_model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\")\n",
    "if GPU:\n",
    "    protbert_model.cuda()\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "x_aminos = re.compile(\"[UZOB]\")\n",
    "\n",
    "def prepare_input(seq, **tokenizer_args):\n",
    "    seq = x_aminos.sub(\"X\", ' '.join(seq))\n",
    "    input_ids = tokenizer.encode(seq, add_special_tokens=True, **tokenizer_args)\n",
    "    return input_ids\n",
    "\n",
    "def get_embeddings(seq, **tokenizer_args):\n",
    "    input_ids = prepare_input(seq, **tokenizer_args)\n",
    "    input_ids = torch.tensor([input_ids], device='cuda' if GPU else 'cpu')\n",
    "    with torch.no_grad():\n",
    "        return protbert_model(input_ids)\n",
    "\n",
    "eg_input = get_embeddings(\"RRWWRRRRW\")\n",
    "eg_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61185c77-fda0-4d24-b105-8f36aacf238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ProteinSequenceDataset(Dataset):\n",
    "    def __init__(self, df, sequence_col='sequence', label_col='family_id', max_len=100):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.sequence_col = sequence_col\n",
    "        self.label_col = label_col\n",
    "        self.max_len = max_len\n",
    "        self._label_translator = {l: torch.tensor(i, device='cuda' if GPU else 'cpu') for i, l in enumerate(sorted(df[label_col].unique()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq, label = row[self.sequence_col], row[self.label_col]\n",
    "        # Convert to tensor\n",
    "        seq = prepare_input(seq[:self.max_len-10], padding='max_length', max_length=self.max_len)\n",
    "        seq = torch.tensor(seq, device='cuda' if GPU else 'cpu')\n",
    "        label = self._label_translator[label]\n",
    "        return seq, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15141a59-5d42-4c64-9e38-3ac9ef56a725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Methyltransf_25', 0),\n",
       " ('LRR_1', 1),\n",
       " ('Acetyltransf_7', 2),\n",
       " ('His_kinase', 3),\n",
       " ('Bac_transf', 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Convert train and test to same top n families\n",
    "top_families = train_df['family_id'].value_counts()[:NUM_CLASSES]\n",
    "# Convert to numbers\n",
    "fam2id = {fam: i for i, fam in enumerate(top_families.index)}\n",
    "list(fam2id.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d8c6e5-3db4-4d27-954e-510a12a12b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 647,248/1,086,741 = 59.558625% of rows due to nan famid num.\n",
      "Removing 71,793/126,171 = 56.901348% of rows due to nan famid num.\n",
      "Removing 71,793/126,171 = 56.901348% of rows due to nan famid num.\n"
     ]
    }
   ],
   "source": [
    "def add_and_filter_family_id(df):\n",
    "    df['family_code'] = df['family_id'].apply(lambda x: fam2id.get(x, np.nan))\n",
    "    logger.info(f'Removing {df[\"family_code\"].isna().sum():,}/{len(df):,} = {df[\"family_code\"].isna().mean()*100:,.6f}% of rows due to nan famid num.')\n",
    "    return df.dropna(subset='family_code').reset_index(drop=True)\n",
    "\n",
    "train_df = add_and_filter_family_id(train_df)\n",
    "test_df = add_and_filter_family_id(test_df)\n",
    "val_df = add_and_filter_family_id(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7de3d469-98c2-47bb-b293-83b96ba054bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((439493, 6), (54378, 6), (54378, 6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc; gc.collect()\n",
    "train_df.shape, test_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a78873-120b-465c-b442-3eacc8ddcd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2, 14, 16,  ...,  0,  0,  0],\n",
       "         [ 2,  6,  5,  ...,  0,  0,  0],\n",
       "         [ 2,  8, 15,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 2, 14, 12,  ...,  0,  0,  0],\n",
       "         [ 2,  5, 19,  ...,  0,  0,  0],\n",
       "         [ 2, 16, 10,  ...,  0,  0,  0]], device='cuda:0'),\n",
       " tensor([433, 289, 117, 749, 215,  85,  59,  31,  63, 185, 494,  46, 986, 298,\n",
       "         383, 107, 865, 314, 164, 441, 337, 199, 413, 348, 320, 367, 688, 223,\n",
       "         618, 392, 541, 906,  65, 454, 831, 423, 229, 259, 465, 468,  14,  43,\n",
       "          11, 178,   8,  81, 417,  33,  55, 489, 721, 962, 875, 601, 174, 350,\n",
       "         747, 161, 257, 367,  35, 277, 228,  64], device='cuda:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ProteinSequenceDataset(train_df, label_col='family_code', max_len=MAX_SEQ_LENGTH)\n",
    "test_dataset  = ProteinSequenceDataset(test_df, label_col='family_code', max_len=MAX_SEQ_LENGTH)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_features, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b661f4-58ff-4fc6-bf3b-f6ddb1291890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer models from tutorial https://n8henrie.com/2021/08/writing-a-transformer-classifier-in-pytorch/\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(vocab_size, d_model)\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Text classifier based on a pytorch TransformerEncoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings,\n",
    "        vocab_size=30,\n",
    "        embedding_size=1024,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        num_layers=6,\n",
    "        num_labels=2,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        classifier_dropout=0.1,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        d_model = embedding_size\n",
    "        assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
    "\n",
    "        self.emb = embeddings\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=d_model,\n",
    "            dropout=dropout,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "        self.d_model = d_model\n",
    "        self._agg_type = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            embeds = self.emb(x)\n",
    "            # x = embeds[:,-10:,:] # Only need last ten\n",
    "            x = embeds\n",
    "        # x = self.emb(x) * math.sqrt(self.d_model)\n",
    "        # x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        if self._agg_type == 0:\n",
    "            x = x[:, -1, :]\n",
    "        else:\n",
    "            x = x.mean(1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8daf6b2c-ca01-4ea8-a85f-16da306e909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = Net(\n",
    "    protbert_model.bert.embeddings,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    nhead=8,  # the number of heads in the multiheadattention models\n",
    "    dim_feedforward=50,  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    num_layers=6,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    dropout=0.2,\n",
    "    classifier_dropout=0.2,\n",
    ").to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    tf_model.parameters(), lr=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29f2d969-34b8-4b00-9b04-6908554c04e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55360f7f-cb55-4b3f-825b-3c4013fd7cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: structlog in ./.local/lib/python3.8/site-packages (22.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "2023-01-21 17:49:56 [info     ] starting\n",
      "2023-01-21 17:49:56 [info     ] epoch=0\n",
      "2023-01-21 17:50:28 [info     ] epoch=0 99 / 6868 epoch_loss/epoch_count=0.095014 107/6,400\n",
      "2023-01-21 17:51:01 [info     ] epoch=0 199 / 6868 epoch_loss/epoch_count=0.094684 241/12,800\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "logger.info(\"starting\")\n",
    "# with torch.autocast(device_type='cuda' if GPU else 'cpu'):\n",
    "for epoch in range(EPOCHS):\n",
    "    logger.info(f\"{epoch=}\")\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_count = 0\n",
    "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = tf_model(inputs)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "\n",
    "        correct = predictions.argmax(axis=1) == labels\n",
    "        acc = correct.sum().item() / correct.size(0)\n",
    "        epoch_correct += correct.sum().item()\n",
    "        epoch_count += correct.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(tf_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 99:\n",
    "            logger.info(f'{epoch=} {idx} / {len(train_dataloader)} {epoch_loss/epoch_count=:,.6f} {epoch_correct:,}/{epoch_count:,}')\n",
    "\n",
    "    logger.info(f\"{epoch=} {epoch_loss=}\")\n",
    "    logger.info(f\"{epoch=} accuracy: {epoch_correct / epoch_count}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_epoch_loss = 0\n",
    "        test_epoch_correct = 0\n",
    "        test_epoch_count = 0\n",
    "\n",
    "        for idx, (inputs, labels) in enumerate(test_dataloader):\n",
    "            predictions = tf_model(inputs)\n",
    "            test_loss = loss_fn(predictions, labels)\n",
    "\n",
    "            correct = predictions.argmax(axis=1) == labels\n",
    "            acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "            test_epoch_correct += correct.sum().item()\n",
    "            test_epoch_count += correct.size(0)\n",
    "            test_epoch_loss += loss.item()\n",
    "\n",
    "    logger.info(f\"{epoch=} {test_epoch_loss=}\")\n",
    "    logger.info(f\"test {epoch=} accuracy: {test_epoch_correct / test_epoch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f3caeee-9f72-4502-873b-929d057a6a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 23,  5,  ...,  0,  0,  0],\n",
      "        [ 2, 10, 15,  ...,  0,  0,  0],\n",
      "        [ 2,  7, 18,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 2,  5, 13,  ...,  0,  0,  0],\n",
      "        [ 2, 12, 12,  ...,  0,  0,  0],\n",
      "        [ 2, 14, 11,  ...,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2.6099,  2.3300,  2.2283,  ..., -1.0133, -0.6502, -2.2384],\n",
      "        [ 2.0139, -2.4519,  0.9142,  ..., -0.1738, -0.1001, -0.9073],\n",
      "        [-0.6249, -4.6749, -1.3534,  ...,  0.6547,  0.3998,  1.4845],\n",
      "        ...,\n",
      "        [ 2.8434,  0.9349,  2.1142,  ..., -0.8581, -0.5297, -2.1230],\n",
      "        [-0.6268, -4.6751, -1.3434,  ...,  0.6404,  0.3866,  1.4682],\n",
      "        [ 2.7412, -0.5299,  1.7431,  ..., -0.6384, -0.3587, -1.7811]],\n",
      "       device='cuda:0')\n",
      "tensor(6.0472, device='cuda:0')\n",
      "tensor([  0,  12, 103, 103,   0,   0,   0,   1,   4,   0,   4,   0, 103,   0,\n",
      "        103,   4,   1,   0,   0,   4,   0, 103,   1,   4,   0,   0,   1,   4,\n",
      "        103,   1,   4,   0,   0,   0, 103,   0,   0, 103,   0, 103,   4,   4,\n",
      "          4,   4,   4,   0,   0,   0,   0,   0,   4,   0,   0, 103, 103,   0,\n",
      "          0,   0,   0,   0,   0,   0, 103,   0], device='cuda:0')\n",
      "tensor([  9, 323, 944, 299, 810, 221, 616, 766, 262,  55,  78, 139, 724,  12,\n",
      "        449, 227, 240,  13,  75, 662, 850, 156, 195, 164, 899, 886, 317,  67,\n",
      "        289, 140, 542, 245, 710, 220, 522, 819,  30,  86, 567, 388, 228,  16,\n",
      "        904,  16, 392, 274, 142, 374, 297, 100, 796, 850, 339, 574, 403,  14,\n",
      "         89, 142,  98, 513,  50, 509, 864, 221], device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2c88090c05e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Verify its not just giving the same label or getting lucky\n",
    "with torch.no_grad():\n",
    "    test_epoch_loss = 0\n",
    "    test_epoch_correct = 0\n",
    "    test_epoch_count = 0\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(test_dataloader):\n",
    "        predictions = tf_model(inputs)\n",
    "        test_loss = loss_fn(predictions, labels)\n",
    "\n",
    "        # correct = predictions.argmax(axis=1) == labels\n",
    "        # acc = correct.sum().item() / correct.size(0)\n",
    "        print(inputs)\n",
    "        print(predictions)\n",
    "        print(test_loss)\n",
    "        print(predictions.argmax(axis=1))\n",
    "        print(labels)\n",
    "        if idx * BATCH_SIZE > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29a6e8-0591-4a91-994e-a9d07d9821c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549c42d-63f6-4846-b999-7cd451151c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
