{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c64a5ac-47f4-488b-afd2-e79a1eac897e",
   "metadata": {},
   "source": [
    "# Model Basline\n",
    "In this notebook we plan to create a continuous-bag-of-words model and feed it into some kind of linear learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7518db13-5a80-475e-9a50-14064afc5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pimports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3564b2e-e909-4e30-99df-9b464f6a62db",
   "metadata": {},
   "source": [
    "## Create CBOW Model\n",
    "\n",
    "To create this with protein data, we will use sequences of 3 amino acids as a word. These are known as k-mers, and here k=3 as is commonly used in research.\n",
    "\n",
    "We will then train the model weights with the kmers (words) surrounding a target kmer (word). From these embeddings we will train a linear learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394afa9d-bca7-4bb0-b015-723c4d7c239f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8aae8838f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Following https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f76523b-6b24-4e6d-9ca5-d379a6046f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequences</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RRWWRRWRRW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GWKSVFRKAKKVGKTVGGLALDHYLG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALWKTMLKKLGTMALHAGKAALGAAADTISQGTQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GLFDVIKKVAAVIGGL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VAKLLAKLAKKVL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>LLKLLKWLLKLLK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>GFKDWIKGAAKKLIKTVAANIANQ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>ESEFDRQEYEECKRQCMQLETSGQMRRCVSQCDKRFEEDIDWSKYDNQE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>FISAIASFLGKFL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>RLSRIVVIRVCR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sequences  label\n",
       "0                                            RRWWRRWRRW      0\n",
       "1                            GWKSVFRKAKKVGKTVGGLALDHYLG      0\n",
       "2                    ALWKTMLKKLGTMALHAGKAALGAAADTISQGTQ      1\n",
       "3                                      GLFDVIKKVAAVIGGL      1\n",
       "4                                         VAKLLAKLAKKVL      1\n",
       "...                                                 ...    ...\n",
       "1373                                      LLKLLKWLLKLLK      0\n",
       "1374                           GFKDWIKGAAKKLIKTVAANIANQ      0\n",
       "1375  ESEFDRQEYEECKRQCMQLETSGQMRRCVSQCDKRFEEDIDWSKYDNQE      0\n",
       "1376                                      FISAIASFLGKFL      1\n",
       "1377                                       RLSRIVVIRVCR      0\n",
       "\n",
       "[1378 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load example protein data to make words\n",
    "prot_df = pd.read_csv(\"data/acp/train_data.csv\")\n",
    "prot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c26a7dc2-6feb-49ba-a5b3-a10b9eb21d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RRW', 17), ('RWW', 5), ('WWR', 4), ('WRR', 11), ('RWR', 10)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn all sequences into kmers (k=3)\n",
    "from collections import Counter  # Might as well count frequency\n",
    "kmers = Counter()\n",
    "for seq in prot_df['sequences'].values:\n",
    "    for kmer_a, kmer_b, kmer_c in zip(seq, seq[1:], seq[2:]):\n",
    "        kmer = ''.join([kmer_a, kmer_b, kmer_c])\n",
    "        kmers[kmer] += 1\n",
    "list(kmers.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da7fc98-3c77-4a45-960d-9cabfd54c455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {kmer: i for i, kmer in enumerate(kmers)}\n",
    "embeds = nn.Embedding(len(word_to_ix), 5)  # x words in vocab, y=5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"RRW\"]], dtype=torch.long)\n",
    "rrw_embed = embeds(lookup_tensor)\n",
    "print(rrw_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebd60897-f98d-4165-95a4-91345deacf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['RRW', 'RWW', 'WRR', 'RRW'], 'WWR'),\n",
       " (['RWW', 'WWR', 'RRW', 'RWR'], 'WRR'),\n",
       " (['WWR', 'WRR', 'RWR', 'WRR'], 'RRW'),\n",
       " (['WRR', 'RRW', 'WRR', 'RRW'], 'RWR'),\n",
       " (['GWK', 'WKS', 'SVF', 'VFR'], 'KSV')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "vocab = set(kmers)\n",
    "vocab_size = len(vocab)\n",
    "kmer_len = 3\n",
    "\n",
    "data = []\n",
    "\n",
    "for seq in prot_df['sequences'].values:\n",
    "    for i in range(0, len(seq) - CONTEXT_SIZE*kmer_len):\n",
    "        kmer_minus_2 = seq[i:i+kmer_len]\n",
    "        kmer_minus_1 = seq[i+1:i+kmer_len+1]\n",
    "        kmer_i = seq[i+2:i+kmer_len+2]\n",
    "        kmer_plus_1 = seq[i+3:i+kmer_len+3]\n",
    "        kmer_plus_2 = seq[i+4:i+kmer_len+4]\n",
    "        data.append(([kmer_minus_2, kmer_minus_1, kmer_plus_1, kmer_plus_2], kmer_i))\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5bb9f0cf-3780-4c92-922c-43c6d03e7c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 24582 rows\n",
      "Removing 0 contexts\n",
      "Now have 24582\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "print(f'Starting with {len(data)} rows')\n",
    "dupes = {}\n",
    "for ctx, wrd in data:\n",
    "    ctx = ','.join(ctx)\n",
    "    if ctx not in dupes:\n",
    "        dupes[ctx] = {wrd}\n",
    "    else:\n",
    "        dupes[ctx].add(wrd)\n",
    "duplicates = {ctx for ctx, wrds in dupes.items() if len(wrds) > 1}\n",
    "print(f'Removing {len(duplicates):,} contexts')\n",
    "\n",
    "data = [(ctx, word) for ctx, word in data if ','.join(ctx) not in duplicates]\n",
    "print(f'Now have {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e825ce99-a346-4d83-a843-e6b3a1bd946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 2**7)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(2**7, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad46ce42-b2a3-4dff-a9b8-941ca4495f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(5647, 100)\n",
       "  (linear1): Linear(in_features=100, out_features=128, bias=True)\n",
       "  (activation_function1): ReLU()\n",
       "  (linear2): Linear(in_features=128, out_features=5647, bias=True)\n",
       "  (activation_function2): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CBOW(vocab_size, 100)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37a75e60-e70f-4ab4-91c5-7040c375d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736155b7-f458-4ee4-b8f8-9a4b40f9e605",
   "metadata": {},
   "source": [
    "I played with the optimizer a lot. With a large lr and SGD, I got NaN loss reliably. I decided to avoid this by using Adam optimizer which allows for weight decay, which tries to minimize the weights. I also found that higher learning rate is fine for a baseline model, but not for a very good one, as there will be lots of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34bf6d4e-0b97-44f3-bf49-f06c3b745d90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "tensor(11826.0049, grad_fn=<AddBackward0>)\n",
      "Done with epoch 0 and got accuracy 1616/4917=32.86557%\n",
      "Starting epoch 1\n",
      "tensor(17810.7676, grad_fn=<AddBackward0>)\n",
      "Done with epoch 1 and got accuracy 1387/4917=28.20826%\n",
      "Starting epoch 2\n",
      "tensor(6046.8428, grad_fn=<AddBackward0>)\n",
      "Done with epoch 2 and got accuracy 1642/4917=33.39435%\n",
      "Starting epoch 3\n",
      "tensor(5740.8506, grad_fn=<AddBackward0>)\n",
      "Done with epoch 3 and got accuracy 1673/4917=34.02481%\n",
      "Starting epoch 4\n",
      "tensor(5520.6245, grad_fn=<AddBackward0>)\n",
      "Done with epoch 4 and got accuracy 1681/4917=34.18751%\n",
      "Starting epoch 5\n",
      "tensor(4224.8203, grad_fn=<AddBackward0>)\n",
      "Done with epoch 5 and got accuracy 1727/4917=35.12304%\n",
      "Starting epoch 6\n",
      "tensor(2669.7395, grad_fn=<AddBackward0>)\n",
      "Done with epoch 6 and got accuracy 1787/4917=36.34330%\n",
      "Starting epoch 7\n",
      "tensor(1649.0242, grad_fn=<AddBackward0>)\n",
      "Done with epoch 7 and got accuracy 1824/4917=37.09579%\n",
      "Starting epoch 8\n",
      "tensor(1032.1907, grad_fn=<AddBackward0>)\n",
      "Done with epoch 8 and got accuracy 1854/4917=37.70592%\n",
      "Starting epoch 9\n",
      "tensor(690.3312, grad_fn=<AddBackward0>)\n",
      "Done with epoch 9 and got accuracy 1855/4917=37.72626%\n",
      "Starting epoch 10\n",
      "tensor(491.3072, grad_fn=<AddBackward0>)\n",
      "Done with epoch 10 and got accuracy 1866/4917=37.94997%\n",
      "Starting epoch 11\n",
      "tensor(334.4634, grad_fn=<AddBackward0>)\n",
      "Done with epoch 11 and got accuracy 1873/4917=38.09233%\n",
      "Starting epoch 12\n",
      "tensor(219.4837, grad_fn=<AddBackward0>)\n",
      "Done with epoch 12 and got accuracy 1875/4917=38.13301%\n",
      "Starting epoch 13\n",
      "tensor(158.9024, grad_fn=<AddBackward0>)\n",
      "Done with epoch 13 and got accuracy 1880/4917=38.23470%\n",
      "Starting epoch 14\n",
      "tensor(124.8635, grad_fn=<AddBackward0>)\n",
      "Done with epoch 14 and got accuracy 1882/4917=38.27537%\n",
      "Starting epoch 15\n",
      "tensor(104.4582, grad_fn=<AddBackward0>)\n",
      "Done with epoch 15 and got accuracy 1882/4917=38.27537%\n",
      "Starting epoch 16\n",
      "tensor(87.5079, grad_fn=<AddBackward0>)\n",
      "Done with epoch 16 and got accuracy 1886/4917=38.35672%\n",
      "Starting epoch 17\n",
      "tensor(79.0022, grad_fn=<AddBackward0>)\n",
      "Done with epoch 17 and got accuracy 1879/4917=38.21436%\n",
      "Starting epoch 18\n",
      "tensor(65.4491, grad_fn=<AddBackward0>)\n",
      "Done with epoch 18 and got accuracy 1880/4917=38.23470%\n",
      "Starting epoch 19\n",
      "tensor(54.5224, grad_fn=<AddBackward0>)\n",
      "Done with epoch 19 and got accuracy 1879/4917=38.21436%\n",
      "Starting epoch 20\n",
      "tensor(50.5455, grad_fn=<AddBackward0>)\n",
      "Done with epoch 20 and got accuracy 1880/4917=38.23470%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#optimize at the end of each epoch\u001b[39;00m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.04, weight_decay=0.001)\n",
    "num_train = int(0.8*len(data))\n",
    "num_test = len(data) - num_train\n",
    "train_data, test_data = torch.utils.data.random_split(data, [num_train, num_test], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "#TRAINING\n",
    "for epoch in range(100):\n",
    "    print(f'Starting epoch {epoch}')\n",
    "    total_loss = 0\n",
    "\n",
    "    for context, target in train_data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)\n",
    "        log_probs = model(context_vector)\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
    "    print(total_loss)\n",
    "    if np.isnan(float(f'{total_loss}')):\n",
    "        break\n",
    "\n",
    "    correct_guesses = 0\n",
    "    for context, target in test_data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)\n",
    "        log_probs = model(context_vector)\n",
    "        guessed = ix_to_word[torch.argmax(log_probs[0]).item()]\n",
    "        if guessed == target:\n",
    "            correct_guesses += 1\n",
    "    print(f'Done with epoch {epoch} and got accuracy {correct_guesses}/{len(test_data)}={100*(correct_guesses / len(test_data)):.5f}%')\n",
    "\n",
    "    #optimize at the end of each epoch\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9bbc1aa1-fabc-4ceb-8907-a09503ddb7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with training and got test accuracy 1880/4917=38.23470% and train accuracy 19647/19665=99.90847%\n"
     ]
    }
   ],
   "source": [
    "# Final loss and accuracy\n",
    "train_guesses = 0\n",
    "for context, target in train_data:\n",
    "    context_vector = make_context_vector(context, word_to_ix)\n",
    "    log_probs = model(context_vector)\n",
    "    guessed = ix_to_word[torch.argmax(log_probs[0]).item()]\n",
    "    if guessed == target:\n",
    "        train_guesses += 1\n",
    "\n",
    "correct_guesses = 0\n",
    "for context, target in test_data:\n",
    "    context_vector = make_context_vector(context, word_to_ix)\n",
    "    log_probs = model(context_vector)\n",
    "    guessed = ix_to_word[torch.argmax(log_probs[0]).item()]\n",
    "    if guessed == target:\n",
    "        correct_guesses += 1\n",
    "print(f'Done with training and got test accuracy {correct_guesses}/{len(test_data)}={100*(correct_guesses / len(test_data)):.5f}% and train accuracy {train_guesses}/{len(train_data)}={100*(train_guesses / len(train_data)):.5f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad03556-6c7a-470e-aafa-9be1d697aa10",
   "metadata": {},
   "source": [
    "38% test accuracy and 99% train accuracy :D Smells like overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc1bd4-80ef-40b4-a33e-cefb84707b1a",
   "metadata": {},
   "source": [
    "## Linear model for guessing label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "730f9cff-376a-4eb4-8256-6e3e2cdb8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(seq):\n",
    "    # Convert to kmers and lookup\n",
    "    kmers = []\n",
    "    for i in range(0, len(seq) - 2):\n",
    "        kmer = seq[i:i+3]\n",
    "        kmers.append(kmer)\n",
    "    return model.embeddings(torch.tensor([word_to_ix[kmer] for kmer in kmers])).cpu().detach().numpy()\n",
    "\n",
    "# Test\n",
    "# get_embeddings('RRWR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d659b8b9-b184-445b-a830-74a2a64c0691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_x</th>\n",
       "      <th>1_x</th>\n",
       "      <th>2_x</th>\n",
       "      <th>3_x</th>\n",
       "      <th>4_x</th>\n",
       "      <th>5_x</th>\n",
       "      <th>6_x</th>\n",
       "      <th>7_x</th>\n",
       "      <th>8_x</th>\n",
       "      <th>9_x</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.444002</td>\n",
       "      <td>0.056918</td>\n",
       "      <td>-0.815464</td>\n",
       "      <td>0.573925</td>\n",
       "      <td>-0.183249</td>\n",
       "      <td>-0.246367</td>\n",
       "      <td>0.569723</td>\n",
       "      <td>0.364270</td>\n",
       "      <td>0.693070</td>\n",
       "      <td>-0.056975</td>\n",
       "      <td>...</td>\n",
       "      <td>7.795949</td>\n",
       "      <td>3.839921</td>\n",
       "      <td>-2.149093</td>\n",
       "      <td>-6.283219</td>\n",
       "      <td>-7.843275</td>\n",
       "      <td>4.828357</td>\n",
       "      <td>3.948138</td>\n",
       "      <td>0.419173</td>\n",
       "      <td>8.552602</td>\n",
       "      <td>-1.960219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.126992</td>\n",
       "      <td>0.049871</td>\n",
       "      <td>0.606737</td>\n",
       "      <td>-0.144443</td>\n",
       "      <td>0.298717</td>\n",
       "      <td>-0.233551</td>\n",
       "      <td>-0.267371</td>\n",
       "      <td>-0.264926</td>\n",
       "      <td>-0.069222</td>\n",
       "      <td>0.448553</td>\n",
       "      <td>...</td>\n",
       "      <td>6.313254</td>\n",
       "      <td>-2.015187</td>\n",
       "      <td>1.417841</td>\n",
       "      <td>9.451605</td>\n",
       "      <td>-5.477207</td>\n",
       "      <td>0.024754</td>\n",
       "      <td>-3.430053</td>\n",
       "      <td>-0.231232</td>\n",
       "      <td>-2.715113</td>\n",
       "      <td>7.010417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.223886</td>\n",
       "      <td>0.235404</td>\n",
       "      <td>-0.050235</td>\n",
       "      <td>0.462699</td>\n",
       "      <td>0.066580</td>\n",
       "      <td>-0.155796</td>\n",
       "      <td>-0.264497</td>\n",
       "      <td>0.078733</td>\n",
       "      <td>-0.048532</td>\n",
       "      <td>0.270114</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.322323</td>\n",
       "      <td>6.068357</td>\n",
       "      <td>-3.053094</td>\n",
       "      <td>-1.743851</td>\n",
       "      <td>-0.013190</td>\n",
       "      <td>-11.695951</td>\n",
       "      <td>-7.152821</td>\n",
       "      <td>9.071996</td>\n",
       "      <td>0.459658</td>\n",
       "      <td>-0.156087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.556409</td>\n",
       "      <td>-0.207510</td>\n",
       "      <td>0.029089</td>\n",
       "      <td>0.204503</td>\n",
       "      <td>0.083392</td>\n",
       "      <td>0.393404</td>\n",
       "      <td>-0.198617</td>\n",
       "      <td>0.778663</td>\n",
       "      <td>0.161003</td>\n",
       "      <td>-0.387152</td>\n",
       "      <td>...</td>\n",
       "      <td>1.107438</td>\n",
       "      <td>-5.195529</td>\n",
       "      <td>0.497815</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>-1.462043</td>\n",
       "      <td>7.551419</td>\n",
       "      <td>3.362416</td>\n",
       "      <td>5.714870</td>\n",
       "      <td>-0.686924</td>\n",
       "      <td>-8.062219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.145889</td>\n",
       "      <td>0.275106</td>\n",
       "      <td>0.067376</td>\n",
       "      <td>-0.089389</td>\n",
       "      <td>-0.116227</td>\n",
       "      <td>-0.378428</td>\n",
       "      <td>-0.191439</td>\n",
       "      <td>0.203924</td>\n",
       "      <td>-0.131348</td>\n",
       "      <td>0.213792</td>\n",
       "      <td>...</td>\n",
       "      <td>3.427410</td>\n",
       "      <td>4.751172</td>\n",
       "      <td>1.452300</td>\n",
       "      <td>5.397296</td>\n",
       "      <td>-2.089801</td>\n",
       "      <td>-1.887212</td>\n",
       "      <td>3.218089</td>\n",
       "      <td>4.966340</td>\n",
       "      <td>-1.001770</td>\n",
       "      <td>-0.731229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>-0.496204</td>\n",
       "      <td>0.157467</td>\n",
       "      <td>0.307309</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>-0.306633</td>\n",
       "      <td>-0.789912</td>\n",
       "      <td>0.297118</td>\n",
       "      <td>0.385418</td>\n",
       "      <td>-0.273252</td>\n",
       "      <td>0.047054</td>\n",
       "      <td>...</td>\n",
       "      <td>3.260774</td>\n",
       "      <td>10.666686</td>\n",
       "      <td>5.799181</td>\n",
       "      <td>-3.966213</td>\n",
       "      <td>5.669536</td>\n",
       "      <td>-2.449130</td>\n",
       "      <td>3.074576</td>\n",
       "      <td>-2.413268</td>\n",
       "      <td>-4.381186</td>\n",
       "      <td>1.743143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>0.009337</td>\n",
       "      <td>-0.427140</td>\n",
       "      <td>0.381318</td>\n",
       "      <td>0.053133</td>\n",
       "      <td>0.120318</td>\n",
       "      <td>0.055844</td>\n",
       "      <td>-0.139122</td>\n",
       "      <td>0.093994</td>\n",
       "      <td>0.039429</td>\n",
       "      <td>0.157610</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.919102</td>\n",
       "      <td>6.566476</td>\n",
       "      <td>-5.235083</td>\n",
       "      <td>-5.783195</td>\n",
       "      <td>-3.001599</td>\n",
       "      <td>-4.548702</td>\n",
       "      <td>-2.995428</td>\n",
       "      <td>-2.787709</td>\n",
       "      <td>-0.486865</td>\n",
       "      <td>1.645817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>-0.151590</td>\n",
       "      <td>0.133433</td>\n",
       "      <td>0.162137</td>\n",
       "      <td>0.189756</td>\n",
       "      <td>0.209770</td>\n",
       "      <td>0.046477</td>\n",
       "      <td>-0.192852</td>\n",
       "      <td>-0.224525</td>\n",
       "      <td>-0.338736</td>\n",
       "      <td>0.124081</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.011648</td>\n",
       "      <td>-10.000915</td>\n",
       "      <td>-12.864613</td>\n",
       "      <td>-10.397711</td>\n",
       "      <td>10.204558</td>\n",
       "      <td>-11.258795</td>\n",
       "      <td>8.158518</td>\n",
       "      <td>8.305094</td>\n",
       "      <td>3.359185</td>\n",
       "      <td>-8.301657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>0.188698</td>\n",
       "      <td>0.049085</td>\n",
       "      <td>0.467198</td>\n",
       "      <td>-0.159195</td>\n",
       "      <td>0.121160</td>\n",
       "      <td>0.396945</td>\n",
       "      <td>-0.483380</td>\n",
       "      <td>0.266270</td>\n",
       "      <td>0.282483</td>\n",
       "      <td>0.095831</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.366853</td>\n",
       "      <td>2.664019</td>\n",
       "      <td>-1.250323</td>\n",
       "      <td>1.958580</td>\n",
       "      <td>-2.589087</td>\n",
       "      <td>-1.681041</td>\n",
       "      <td>5.271566</td>\n",
       "      <td>0.630387</td>\n",
       "      <td>-3.710766</td>\n",
       "      <td>-3.578943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>0.227965</td>\n",
       "      <td>-0.405863</td>\n",
       "      <td>-0.235844</td>\n",
       "      <td>0.505946</td>\n",
       "      <td>-0.086601</td>\n",
       "      <td>-0.138654</td>\n",
       "      <td>0.251235</td>\n",
       "      <td>-0.085655</td>\n",
       "      <td>-0.689960</td>\n",
       "      <td>-0.043371</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.713783</td>\n",
       "      <td>5.572189</td>\n",
       "      <td>-2.972149</td>\n",
       "      <td>-3.912648</td>\n",
       "      <td>3.083359</td>\n",
       "      <td>-2.327823</td>\n",
       "      <td>3.317887</td>\n",
       "      <td>3.862004</td>\n",
       "      <td>1.920710</td>\n",
       "      <td>-3.166190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0_x       1_x       2_x       3_x       4_x       5_x       6_x  \\\n",
       "0     0.444002  0.056918 -0.815464  0.573925 -0.183249 -0.246367  0.569723   \n",
       "1     0.126992  0.049871  0.606737 -0.144443  0.298717 -0.233551 -0.267371   \n",
       "2    -0.223886  0.235404 -0.050235  0.462699  0.066580 -0.155796 -0.264497   \n",
       "3     0.556409 -0.207510  0.029089  0.204503  0.083392  0.393404 -0.198617   \n",
       "4     0.145889  0.275106  0.067376 -0.089389 -0.116227 -0.378428 -0.191439   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1373 -0.496204  0.157467  0.307309  0.005698 -0.306633 -0.789912  0.297118   \n",
       "1374  0.009337 -0.427140  0.381318  0.053133  0.120318  0.055844 -0.139122   \n",
       "1375 -0.151590  0.133433  0.162137  0.189756  0.209770  0.046477 -0.192852   \n",
       "1376  0.188698  0.049085  0.467198 -0.159195  0.121160  0.396945 -0.483380   \n",
       "1377  0.227965 -0.405863 -0.235844  0.505946 -0.086601 -0.138654  0.251235   \n",
       "\n",
       "           7_x       8_x       9_x  ...        90         91         92  \\\n",
       "0     0.364270  0.693070 -0.056975  ...  7.795949   3.839921  -2.149093   \n",
       "1    -0.264926 -0.069222  0.448553  ...  6.313254  -2.015187   1.417841   \n",
       "2     0.078733 -0.048532  0.270114  ... -5.322323   6.068357  -3.053094   \n",
       "3     0.778663  0.161003 -0.387152  ...  1.107438  -5.195529   0.497815   \n",
       "4     0.203924 -0.131348  0.213792  ...  3.427410   4.751172   1.452300   \n",
       "...        ...       ...       ...  ...       ...        ...        ...   \n",
       "1373  0.385418 -0.273252  0.047054  ...  3.260774  10.666686   5.799181   \n",
       "1374  0.093994  0.039429  0.157610  ... -6.919102   6.566476  -5.235083   \n",
       "1375 -0.224525 -0.338736  0.124081  ... -2.011648 -10.000915 -12.864613   \n",
       "1376  0.266270  0.282483  0.095831  ... -6.366853   2.664019  -1.250323   \n",
       "1377 -0.085655 -0.689960 -0.043371  ... -5.713783   5.572189  -2.972149   \n",
       "\n",
       "             93         94         95        96        97        98        99  \n",
       "0     -6.283219  -7.843275   4.828357  3.948138  0.419173  8.552602 -1.960219  \n",
       "1      9.451605  -5.477207   0.024754 -3.430053 -0.231232 -2.715113  7.010417  \n",
       "2     -1.743851  -0.013190 -11.695951 -7.152821  9.071996  0.459658 -0.156087  \n",
       "3      0.000678  -1.462043   7.551419  3.362416  5.714870 -0.686924 -8.062219  \n",
       "4      5.397296  -2.089801  -1.887212  3.218089  4.966340 -1.001770 -0.731229  \n",
       "...         ...        ...        ...       ...       ...       ...       ...  \n",
       "1373  -3.966213   5.669536  -2.449130  3.074576 -2.413268 -4.381186  1.743143  \n",
       "1374  -5.783195  -3.001599  -4.548702 -2.995428 -2.787709 -0.486865  1.645817  \n",
       "1375 -10.397711  10.204558 -11.258795  8.158518  8.305094  3.359185 -8.301657  \n",
       "1376   1.958580  -2.589087  -1.681041  5.271566  0.630387 -3.710766 -3.578943  \n",
       "1377  -3.912648   3.083359  -2.327823  3.317887  3.862004  1.920710 -3.166190  \n",
       "\n",
       "[1378 rows x 300 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prot_df['embeddings'] = prot_df['sequences'].map(get_embeddings)\n",
    "\n",
    "prot_df['mean_embeddings'] = prot_df['embeddings'].map(lambda x: x.mean(0))\n",
    "prot_df['max_embeddings'] = prot_df['embeddings'].map(lambda x: x.max(0))\n",
    "prot_df['sum_embeddings'] = prot_df['embeddings'].map(lambda x: x.sum(0))\n",
    "\n",
    "mean_embeddings = pd.DataFrame(prot_df['mean_embeddings'].tolist())\n",
    "max_embeddings = pd.DataFrame(prot_df['max_embeddings'].tolist())\n",
    "sum_embeddings = pd.DataFrame(prot_df['sum_embeddings'].tolist())\n",
    "all_embeddings = mean_embeddings.merge(max_embeddings, left_index=True, right_index=True).merge(sum_embeddings, left_index=True, right_index=True)\n",
    "all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8439a-da04-4a54-84ce-ad5ad9a6bb0f",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8ddbcff-638d-44f9-b0d2-264c9c310a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jong/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72       150\n",
      "           1       0.67      0.68      0.67       126\n",
      "\n",
      "    accuracy                           0.70       276\n",
      "   macro avg       0.70      0.70      0.70       276\n",
      "weighted avg       0.70      0.70      0.70       276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jong/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=1_000, max_depth=10, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = all_embeddings, prot_df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit RandomForestClassifier\n",
    "rfc.fit(X_train, y_train)\n",
    "# Predict the test set labels\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Analyze results\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d').set_title('confusion matrix')\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeef61d-2100-4c3a-b9c5-cfbc4de32622",
   "metadata": {},
   "source": [
    "Around 70% accuracy. That's pretty good for a baseline! Especially with a very overfitting embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add16c0-cb78-4046-93c2-adea75aa155d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
